
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex

% Set distance between figures and text
\setlength\floatsep{2\baselineskip}
\setlength\textfloatsep{2\baselineskip}
\setlength\intextsep{2\baselineskip}

% TikZ is used for diagrams
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,fit}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable


\parindent 0pt % Disable paragraph indent and add paragraph spacing
\parskip 6pt

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Oliver Lane}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Audio Fingerprinting for Music Recongition} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity Hall \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Oliver Lane                       \\
College:            & \bf Trinity Hall                     \\
Project Title:      & \bf Audio Fingerprinting for Music Recognition \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2015  \\
Word count:         & \bf   \\
Project Originator: & Oliver Lane                    \\
Supervisor:         & Vaiva Imbrasait\'{e}                    \\ 
\end{tabular}
}


\section*{Original Aims of the Project}

In this project I aimed firstly to implement at least two audio fingerprinting algorithms, each with a corresponding matching algorithm, for the purpose of recognising clips of songs from a library in a way that is robust to noise and distortions.

Secondly, I aimed to assemble a library of songs and corresponsing test clips against which to test these algorithms.

Finally, I aimed to compare these implementations against several criteria, including size of fingerprints generated and percentage of test clips matched correctly from the assembled library, at various clip lengths and levels of noise.

\section*{Work Completed}

Two algorithms for audio fingerprinting were successfully implemented, with corresponsing matching algorithms. These algorithms were proposed by Avery Wang in 2003 \cite{Wang03} and Haitsma et al. in 2002 \cite{Haitsma02}. A library of songs was assembled, and corresponding test cases made by taking different length clips from the library and adding distortions.

The two implementations were tested and evaluated against several criteria using the set of test clips, and compared against each other.

\section*{Special Difficulties}

None.

 
\newpage
\section*{Declaration}

I, Oliver Lane of Trinity Hall, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in  it are my own work, unaided except as may be specified below, and that the  dissertation does not contain material that has already been used to any  substantial extent for a comparable purpose.

\bigskip
\leftline{Signed}

\medskip
\leftline{Date}

\tableofcontents

%\listoffigures


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              CHAPTERS

\pagestyle{headings}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

This project concerns the problem of identifying songs from small fragments of audio. Algorithms used to tackle this problem are usually called audio fingerprinting algorithms. In this project, I successfully implement two such algorithms, and compare their relative strengths and weaknesses.

This section serves as an introduction to the subject area, and summarises the main content of the project.

\section{Introduction to audio fingerprinting}

Audio fingerprinting gives the ability to identify metadata for songs, videos and other media based on the content of the audio signal (or, often, a small section of that audio).

This technology has found a variety of uses in both industry and consumer products. For example, a user might want to identify a song's name and artist by recording a clip of it using their mobile phone. Music labels might use the technology to automatically monitor radio stations to ensure the correct song royalties are being paid, or to use as part of a watermarking system to track copyright violations.


\subsection{Fingerprinting functions}

An audio fingerprint is a compact digital representation of some audio, which summarises its content. It is generated deterministically from the audio signal, and can be thought of as a kind of hash, where the aim is that perceptually similar pieces of audio -- that is, audio which sounds the same or very similar to the human ear -- have similar hash values.

An audio fingerprinting algorithm needs to work independently to the representation of the audio signal. If two audio signals sound the same to the human ear then their fingerprints should be a close match, regardless of how similar the binary representation of the audio was. For instance, two copies of the same song encoded at different rates of compression will have quite different representations but are very perceptually similar, and so should have very similar fingerprints.

As a result, a robust audio fingerprinting algorithm needs to take into account the perceptual characteristics of the audio, and be as unaffected as possible in the face of likely distortions. Despite this, it must retain enough discriminatory power to distinguish between similar songs.

Once an audio fingerprint has been computed for a fragment of audio, a precomputed database of song fingerprints can be searched for a close match. Here, a close match is defined by some distance metric which defines how similar two fingerprints are determined to be. If a close match is found, we conclude that our fragment of audio is from the song found in the database.

Note that it is not possible to create a fingerprint function which always results in identical fingerprints for perceptually similar audio, because perceptual similarity is known not to be transitive. That is, if an audio clip A is perceptually similar to two other clips, B and C, then B and C are not necessarily perceptually similar. A fingerprinting scheme which relied on absolute equality would enforce such a property, and would therefore be unsuitable for modelling perceptual similarity.

In conclusion, when designing a fingerprinting function, we require that two perceptually similar audio objects result in similar fingerprints. In addition, we require that the probability that two dissimilar audio objects are very unlikely to produce similar fingerprints. 

More formally, we require that a well designed fingerprint function $F$ and corresponding distance function $D$ will have some threshold $t$ such that with very high probability $D(F(A),F(B)) <= t$ if objects A and B are perceptually similar, and $D(F(A),F(B)) > t$ if they are not.


\subsection{General framework}

Despite their differences in approach, most audio fingerprinting algorithms share a common framework.

Each algorithm consists of three main parts: 

\begin{itemize}
  \item A fingerprinter, which generates an audio fingerprint from a given piece of audio
  \item A registrar, which builds a database of fingerprints from a library of song audio files and their metadata
  \item A matcher, which matches a fingerprint against the precomputed database
\end{itemize}

Given that the matcher needs to search the database for close matches, some notion of distance is needed to decide how similar two fingerprints are. Since a library can grow to many thousands of songs in size, and distance comparisons can be computationally expensive, methods of speeding up the database search are often employed. This often involves using a cheaper distance measure first to discard unlikely candidates, before using a more accurate measure to hone in on matches.


\begin{figure}[h]
  \centering
  \input{figs/general_framework}
  \caption{General structure of an audio fingerprinting algorithm}
\end{figure}


\subsection{System parameters}
\label{section:systemparams}

Audio fingerprinting systems can be evaluated over a variety of parameters, which vary in importance based on the application. The main parameters are summarised below.

\subsubsection{Robustness}

Robustness to signal degradations is one of the most important parameters. Ideally, we would prefer that even severly degraded audio gives a very similar fingerprint. All algorithms require some robustness to resampling and mild compression, since there is no guarantee that the input audio will be in the same format and coded at the same compression rate as the original copy in the library.

Most algorithms will also try to be as robust as possible to other distortions such as echo, reverb, severe compression, background noise and equalisation.

In order to remain robust to distortions, algorithms must base their fingerprints on features of the audio which are reasonably invariant to the distortions in question. Usually these are perceptual features such as frequency peaks.

\subsubsection{Reliability}

Reliability has to do with the proportion of songs which are incorrectly matched. The rate at which this occurs is usually referred to as the false positive rate or the false match rate.

This would be slightly more important, for example, to a music label tracking royalties than to a user finding song titles using their smartphone. However, this metric is always fairly important.

\subsubsection{Fingerprint size}

The size of the fingerprint calculated is important, because to identify metadata we need to create a database of song fingerprints. The larger each fingerprint is in size, the faster this database will grow.

Fingerprint size is usually expressed in bits per second of audio.

\subsubsection{Granularity}

The property of granularity determines how many seconds of audio are required to identify a match in the database. In some applications the whole song can be used for identification, in others shorter clips are preferred. 

The granularity will often depend on the types of degradation applied to the audio; many algorithms can compensate for more severly degraded audio by matching on longer clips.

\subsubsection{Scalability}

Scalability refers to how easily the system can be extended for larger and larger library sizes. A larger library means a larger fingerprint database, and therefore longer search times. Efficient database search strategies are usually employed to keep search times as low as possible.


\section{This project}

There are a many algorithms for audio fingerprinting which have been proposed in the literature, some of which have been used extensively for commercial applications. 

Two such algorithms were selected and implemented for the project.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preparation}

This section will summarise the work that was done before implementation of any code began. Broadly, this encompasses research on the literature around the topic, learning of new skills to be used during implementation, and planning of the implementation and evaluation portions of the project.


\section{Review of audio fingerprinting techniques}

The first step undertaken was to review the current literature around the area. One particularly useful review was given by Cano et al. \cite{Cano02}, which gives a good overview of the general structure of most audio fingerprinting algorithms. This gave me a good starting point for my system design.


\section{Algorithm selection}
\label{section:algoselection}

In selecting the algorithms to compare, I wanted two algorithms which were:

\begin{itemize}
  \item Reasonably similar in their aims, so that they could be compared fairly
  \item Reasonably different in their approach, so that there was still something to compare
\end{itemize}

There are many parameters affecting the design and performance of an audio fingerprinting system, which are described in more detail in section \ref{section:systemparams}. One of the most important, and most studied, is how robust the system is to degradation of input audio through various distortions. 

The specific distortions that an algorithm needs to be robust towards are somewhat dependent on the intended use cases. For example, a system intented for radio broadcast monitoring might need to be robust against radio noise or compression, but wouldn't need to worry about reverb or matching using very short clips. However, a system intended to match songs from a smartphone app would require robustness to background noise (such as ambient crowd noise) and reverb, and would benefit from being able to work using clips which were as short as possible.

The latter use case imposes more constraints on the problem in terms of severity of distortion, especially given the commodity hardware involved. As such, it is generally more interesting from a signal processing perspective, and both algorithms selected for this project try to tackle this use case.

The first algorithm I selected was an algorithm developed by Avery Wang and deployed in the smartphone app Shazam \cite{Wang03}. I chose this as a good algorithm to implement first, since it is 

% TODO

% I also focussed on fully understanding the two algorithms which I had considered during my proposal \cite{Haitsma02} \cite{Wang03}. Both use many of the techniques I have learnt in Part II courses, particularly from Digital Signal Processing, which meant I was well placed to implement them well. However, they also have interesting differences in their approaches, and so they were chosen as the two algorithms to be implemented for this project.


\section{Evaluation}

\subsection{Library}

To test the audio fingerprinting algorithms, a reasonably large library of songs was required to match against. Both of the papers I was focussing on tested their algorithms on databases of 10,000 songs, but a library this size would be difficult for me to obtain for this project. A library size of at least 500 songs was decided, as a compromise between time constraints and the quality of the testing.

Genre and style diversity in the song library was also an important consideration, in order to reduce biases in the testing process.

My own local music library was used as the basis for my test set (around 600 songs). Fortunately my library is fairly diverse, but I also supplemented the library with more songs from the Free Music Archive to increase the representation of less common genres in my local collection. The full test library consists of 720 songs.

\subsection{Test types}

Testing the algorithms consists of inputting clips of known songs in the library, and recording whether each clip was matched to the correct song. Different distortions are applied to these clips so as to test the algorithms' resilience to different types of distortions in the input signal.

Three main categories of test clips were chosen, which are summarised below.

\subsubsection{Plain clips}

Plain clips are simply generated by cropping original song audio files to the required length, and are used as a first check during development to make sure the algorithm is functioning at a basic level. The match rate for these should be 100\%.

\subsubsection{Clips with added noise}

Clips with added noise are generated by taking plain clips and overlaying noise at different signal to noise ratios. The noise added might be generated, such as gaussian noise, or taken from a pre-recorded noise file. An example of pre-recorded noise might be a clip of ambient noise in a busy restaurant.

\subsubsection{Re-recorded clips}

Finally, a re-recorded clip is generated by playing a plain clip out of some speakers and re-recording the audio through a microphone. This captures more naturalistic distortions such as echo and reverb, as well as any inaccuracies, frequency biases or clipping introduced by the microphone or speakers.


\section{Software engineering principles}

\subsection{Technology choices}

One of my early focusses was deciding on the technologies to be used for the project. My main considerations when making these decisions were ease of development, ease of evaluation, and my own familiarity.

To make development as easy as possible, I decided to make sure there was strong library support for operations which would be useful but could take a disproportionate amount of time to implement well myself, such as audio file manipulation. For example, a fast and accurate Discrete Fourier Transform implementation was essential.

I investigated several toolkits for music information retrieval, including in particular the C++ library OpenSMILE \cite{Eyben10} and the MATLAB library MIRToolbox \cite{Lartillot07}.

Ease of evaluation was also important. Although there wouldn't be much difference in writing test scripts in most programming languages, some languages, such as MATLAB, provide inbuilt support for plotting graphs, which could speed up my workflow significantly.

I was already somewhat familiar with MATLAB, having written a little for the Part 1A NST Mathematics course, and for various exercises in Part 1B. I was also familiar with C++ through the Part 1B course. However, I would not have described myself as experienced with either language.

The toolkits for both languages appeared to be roughly comparable for my needs. On balance, I decided that MATLAB was the better choice, mainly on the merit of its strong inbuilt library support for both graph output and common signal processing operations.


\subsection{Version control}

Git was used as my version control system during development and writeup. This provided a backup of all code via GitHub, as well as the ability to roll back changes and develop new features on separate branches. The commit history was also very useful as a supplement to my written notes when writing up the project.


\subsection{Backup strategy}

Development was carried out on my laptop. Regular full disk backups of the machine were taken using Time Machine onto an external drive, in case of corruption or malfunction. In addition, in case of theft or loss of both the laptop and hard drive, all data was backed up remotely. This was achieved using Google Drive for the music library, and a private GitHub repository for code and documentation.


\subsection{Development strategy}

%TODO


\section{Technology familiarisation}

\subsection{MATLAB}

Since I had limited experience using MATLAB, particularly for larger programs, I began by re-familiarising myself with the language. I did this mainly by working through small textbook-style exercises -- for example, implementing bubble sort -- and reading documentation for areas of the language I hadn't previously been exposed to such as namespacing, first-class functions and anonymous functions.

\subsection{MIRToolbox}

I also familiarised myself with MIRToolbox and its documentation. I started to investigate how useful it would be by importing some audio from an mp3 file and drawing a spectrogram of it. I implemented this task both with and without MIRToolbox, to compare the perfromance and ease of development. I found that the MIRToolbox version of the code was both slower and harder to develop, in that MIRToolbox tends to constrain you to a particular data flow to optimise certain operations, which makes it less flexible.

After consulting the operations available in the MATLAB standard library, I decided it would probably be possible to implement the algorithms just as easily without the use of MIRToolbox. Since, based on my preliminary tests, the MIRToolbox code might also be slower, I decided not to use MIRToolbox for my implementations.

\subsection{Database access}

Early on, I identified database access as a possible difficulty with my algorithm implementations. To ensure this would not be a problem, I decided to finish my familiarisation period by setting up a small database and making sure I could read and write to it.

Although MATLAB has an official Database Toolbox for database access, it is not included under the University's blanket license, so I investigated alternatives before attempting to obtain a license by other means.

I found an open source MATLAB SQLite3 driver \cite{Yamaguchi14} which worked well, and supported the full SQLite feature set, including batching multiple operations into transactions. After reading and writing to a small test database, I decided to use this database system for both my algorithm implementations.

%TODO? \section{Starting point}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}

In this section I will outline all of the steps I took to successfully implement the two selected audio fingerprinting algorithms. This starts with the design of the system (section \ref{section:systemdesign}), and then moves on to implementation of each of the Shazam and Philips algorithms (sections \ref{section:shazam} and \ref{section:philips} respectively).


\section{System design}
\label{section:systemdesign}

The first step of implementation was thorough planning in order to ensure a smooth development process. 

Audio fingerprinting naturally divides into two processes: building a database of song fingerprints, and matching unlabelled audio against this database. I will refer to the former as `registration', and the latter as `matching'.

Since matching also involves calculating a fingerprint, the fingerprinting algorithm can be extracted conveniently into its own module, which can be called by both the matching and registration processes.

The code for actual registration, matching and fingerprinting is specific to each of the algorithms implemented, due to significant differences in their approaches to the problem. However, a significant amount of shared code was also required to feed data into the algorithms and collect results.


\subsection{Registration}

\begin{figure}[h]
  \centering
  \input{figs/build_architecture}
  \caption{Block level design of the song registration system.}
  \medskip \small
  The grey blocks within the matching section are algorithm-specific, while the blue blocks are shared code.
\end{figure}


\subsection{Matching}

\begin{figure}[h]
  \centering
  \input{figs/test_architecture}
  \caption{Block level design of the testing system}
  \medskip \small
  The grey blocks within the matching section are algorithm-specific, while the blue blocks are shared code. The test producer can be swapped out to run different test types.
\end{figure}


\section{Implementation of Shazam algorithm}
\label{section:shazam}

\subsection{Fingerprint function}
\subsection{Database}
\subsection{Registration}
\subsection{Matching}

\section{Implementation of Philips algorithm}
\label{section:philips}

\subsection{Fingerprint function}
\subsection{Database}
\subsection{Registration}
\subsection{Matching}





- remember to include thoughts about testing at some point

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Evaluation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}

In this project I successfully implemented two algorithms for audio fingerprinting and demonstrated their use for song recognition. I gathered a library of songs to test against, and used it to evaluate and compare both algorithms against a variety of criteria.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix


\chapter{Project Proposal}

\input{proposal}

\end{document}
