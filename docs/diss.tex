
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage[T1]{fontenc}
\usepackage{amsmath}

% Set distance between figures and text
\setlength\floatsep{2\baselineskip}
\setlength\textfloatsep{2\baselineskip}
\setlength\intextsep{2\baselineskip}

% TikZ is used for diagrams
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,fit}

% Matlab-prettifier is used for matlab code formatting
\usepackage[numbered,framed]{matlab-prettifier}
\lstset{
  style              = Matlab-editor,
  basicstyle         = \mlttfamily,
  mlshowsectionrules = true,
}


\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable


\parindent 0pt % Disable paragraph indent and add paragraph spacing
\parskip 6pt

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Oliver Lane}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Audio Fingerprinting for Music Recongition} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity Hall \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Oliver Lane                       \\
College:            & \bf Trinity Hall                     \\
Project Title:      & \bf Audio Fingerprinting for Music Recognition \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2015  \\
Word count:         & \bf   \\
Project Originator: & Oliver Lane                    \\
Supervisor:         & Vaiva Imbrasait\'{e}                    \\ 
\end{tabular}
}


\section*{Original Aims of the Project}

In this project I aimed firstly to implement at least two audio fingerprinting algorithms, each with a corresponding matching algorithm, for the purpose of recognising clips of songs from a library in a way that is robust to noise and distortions.

Secondly, I aimed to assemble a library of songs and corresponsing test clips against which to test these algorithms.

Finally, I aimed to compare these implementations against several criteria, including size of fingerprints generated and percentage of test clips matched correctly from the assembled library, at various clip lengths and levels of noise.

\section*{Work Completed}

Two algorithms for audio fingerprinting were successfully implemented, with corresponsing matching algorithms. These algorithms were proposed by Avery Wang in 2003 \cite{Wang03} and Haitsma et al. in 2002 \cite{Haitsma02}. A library of songs was assembled, and corresponding test cases made by taking different length clips from the library and adding distortions.

The two implementations were tested and evaluated against several criteria using the set of test clips, and compared against each other.

\section*{Special Difficulties}

None.

 
\newpage
\section*{Declaration}

I, Oliver Lane of Trinity Hall, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in  it are my own work, unaided except as may be specified below, and that the  dissertation does not contain material that has already been used to any  substantial extent for a comparable purpose.

\bigskip
\leftline{Signed}

\medskip
\leftline{Date}

\tableofcontents

%\listoffigures


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              CHAPTERS

\pagestyle{headings}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{introduction}

This project concerns the problem of identifying songs from small fragments of audio. Algorithms used to tackle this problem are usually called audio fingerprinting algorithms. In this project, I successfully implement two such algorithms, and compare their relative strengths and weaknesses.

This section serves as an introduction to the subject area, and summarises the main content of the project.

\section{Introduction to audio fingerprinting}

Audio fingerprinting gives the ability to identify metadata for songs, videos and other media based on the content of the audio signal (or, often, a small section of that audio).

This technology has found a variety of uses in both industry and consumer products. For example, a user might want to identify a song's name and artist by recording a clip of it using their mobile phone. Music labels might use the technology to automatically monitor radio stations to ensure the correct song royalties are being paid, or to use as part of a watermarking system to track copyright violations.


\subsection{Fingerprinting functions}

An audio fingerprint is a compact digital representation of some audio, which summarises its content. It is generated deterministically from the audio signal, and can be thought of as a kind of hash, where the aim is that perceptually similar pieces of audio -- that is, audio which sounds the same or very similar to the human ear -- have similar hash values.

An audio fingerprinting algorithm needs to work independently to the representation of the audio signal. If two audio signals sound the same to the human ear then their fingerprints should be a close match, regardless of how similar the binary representation of the audio was. For instance, two copies of the same song encoded at different rates of compression will have quite different representations but are very perceptually similar, and so should have very similar fingerprints.

As a result, a robust audio fingerprinting algorithm needs to take into account the perceptual characteristics of the audio, and be as unaffected as possible in the face of likely distortions. Despite this, it must retain enough discriminatory power to distinguish between similar songs.

Once an audio fingerprint has been computed for a fragment of audio, a precomputed database of song fingerprints can be searched for a close match. Here, a close match is defined by some distance metric which defines how similar two fingerprints are determined to be. If a close match is found, we conclude that our fragment of audio is from the song found in the database.

Note that it is not possible to create a fingerprint function which always results in identical fingerprints for perceptually similar audio, because perceptual similarity is known not to be transitive. That is, if an audio clip A is perceptually similar to two other clips, B and C, then B and C are not necessarily perceptually similar. A fingerprinting scheme which relied on absolute equality would enforce such a property, and would therefore be unsuitable for modelling perceptual similarity.

In conclusion, when designing a fingerprinting function, we require that two perceptually similar audio objects result in similar fingerprints. In addition, we require that the probability that two dissimilar audio objects are very unlikely to produce similar fingerprints. 

More formally, we require that a well designed fingerprint function $F$ and corresponding distance function $D$ will have some threshold $t$ such that with very high probability $D(F(A),F(B)) <= t$ if objects A and B are perceptually similar, and $D(F(A),F(B)) > t$ if they are not.


\subsection{General framework}

Despite their differences in approach, most audio fingerprinting algorithms share a common framework.

Each algorithm consists of three main parts: 

\begin{itemize}
  \item A fingerprinter, which generates an audio fingerprint from a given piece of audio
  \item A registrar, which builds a database of fingerprints from a library of song audio files and their metadata
  \item A matcher, which matches a fingerprint against the precomputed database
\end{itemize}

This structure is shown in more detail in Figure \ref{fig:generalframework}. 

\begin{figure}[h]
  \centering
  \input{figs/general_framework}
  \caption{General structure of an audio fingerprinting algorithm}
  \label{fig:generalframework}
\end{figure}

Given that the matcher needs to search the database for close matches, some notion of distance is needed to decide how similar two fingerprints are. A library can grow to many thousands of songs in size, and distance comparisons can be computationally expensive, so methods of speeding up the database search are often employed. This often involves using a cheaper distance measure first to discard unlikely candidates, before using a more accurate measure to hone in on matches.


\subsection{System parameters}
\label{section:systemparams}

Audio fingerprinting systems can be evaluated over a variety of parameters, which vary in importance based on the application. The main parameters are summarised below.

\subsubsection{Robustness}

Robustness to signal degradations is one of the most important parameters. Ideally, we would prefer that even severly degraded audio gives a very similar fingerprint. All algorithms require some robustness to resampling and mild compression, since there is no guarantee that the input audio will be in the same format and coded at the same compression rate as the original copy in the library.

Most algorithms will also try to be as robust as possible to other distortions such as echo, reverb, severe compression, background noise and equalisation.

In order to remain robust to distortions, algorithms must base their fingerprints on features of the audio which are reasonably invariant to the distortions in question. Usually these are perceptual features such as frequency peaks.

\subsubsection{Reliability}

Reliability has to do with the proportion of songs which are incorrectly matched. The rate at which this occurs is usually referred to as the false positive rate or the false match rate.

This would be slightly more important, for example, to a music label tracking royalties than to a user finding song titles using their smartphone. However, this metric is always fairly important.

\subsubsection{Fingerprint size}

The size of the fingerprint calculated is important, because to identify metadata we need to create a database of song fingerprints. The larger each fingerprint is in size, the faster this database will grow.

Fingerprint size is usually expressed in bits per second of audio.

\subsubsection{Granularity}

The property of granularity determines how many seconds of audio are required to identify a match in the database. In some applications the whole song can be used for identification, in others shorter clips are preferred. 

The granularity will often depend on the types of degradation applied to the audio; many algorithms can compensate for more severly degraded audio by matching on longer clips.

\subsubsection{Scalability}

Scalability refers to how easily the system can be extended for larger and larger library sizes. A larger library means a larger fingerprint database, and therefore longer search times. Efficient database search strategies are usually employed to keep search times as low as possible.


\section{This project}

There are a many algorithms for audio fingerprinting which have been proposed in the literature, some of which have been used extensively for commercial applications. 

Two such algorithms were selected and implemented for the project.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preparation}
\label{preparation}

The next three sections will cover all of the work done for the project. This section will summarise the work that was done before implementation of any code began. Broadly, this encompasses research on the literature around the topic, learning of new skills to be used during implementation, and planning of the implementation and evaluation portions of the project. It also involved the selection of two algorithms to be implemented and compared.

Sections \ref{implementation} and \ref{evaluation} will cover the actual implementation of the two algorithms selected, and the subsequent evaluation of those implementations against each other.


\section{Review of audio fingerprinting techniques}

The first step undertaken was to review the current literature around the area. One particularly useful review was given by Cano et al. \cite{Cano02}, which gives a good overview of the general structure of most audio fingerprinting algorithms. This gave me a good starting point for my system design.


\section{Algorithm selection}
\label{section:algoselection}

In selecting the algorithms to compare, I wanted two algorithms which were:

\begin{itemize}
  \item Reasonably similar in their aims, so that they could be compared fairly
  \item Reasonably different in their approach, so that there was still something to compare
\end{itemize}

There are many parameters affecting the design and performance of an audio fingerprinting system, which are described in more detail in section \ref{section:systemparams}. One of the most important, and most studied, is how robust the system is to degradation of input audio through various distortions. 

The specific distortions that an algorithm needs to be robust towards are somewhat dependent on the intended use cases. For example, a system intented for radio broadcast monitoring might need to be robust against radio noise or compression, but wouldn't need to worry about reverb or matching using very short clips. However, a system intended to match songs from a smartphone app would require robustness to background noise (such as ambient crowd noise) and reverb, and would benefit from being able to work using clips which were as short as possible.

The latter use case imposes more constraints on the problem in terms of severity of distortion, especially given the commodity hardware involved. As such, it is generally more interesting from a signal processing perspective, and both algorithms selected for this project try to tackle this use case.

The first algorithm I selected was an algorithm developed by Avery Wang in 2003 and deployed in the smartphone app Shazam \cite{Wang03}. I chose this as a good algorithm to implement first, since it is well documented both in the literature and on several blogs and forums. I will refer to this as the \textit{Shazam algorithm}.

The second algorithm selected was an algorithm proposed by Philips researchers Haitsma et al. in 2002 \cite{Haitsma02}. I will refer to this as the \textit{Philips algorithm}. Although both algorithms begin with a similar step of taking the Fourier transform of the audio, the Shazam algorithm extracts local frequency-time domain maxima as its features, whilst the Philips algorithm computes a calculation based on the energy in different frequency bands. I decided that this pair represented a reasonable choice regarding the two aims stated above.


\section{Evaluation}

\subsection{Library}

To test the audio fingerprinting algorithms, a reasonably large library of songs was required to match against. Both of the papers I was focussing on tested their algorithms on databases of 10,000 songs, but a library this size would be difficult for me to obtain for this project. A library size of at least 500 songs was decided, as a compromise between time constraints and the quality of the testing.

Genre and style diversity in the song library was also an important consideration, in order to reduce biases in the testing process.

My own local music library was used as the basis for my test set (around 600 songs). Fortunately my library is fairly diverse, but I also supplemented the library with more songs from the Free Music Archive to increase the representation of less common genres in my local collection. The full test library consists of 720 songs.

\subsection{Test types}

Testing the algorithms consists of inputting clips of known songs in the library, and recording whether each clip was matched to the correct song. Different distortions are applied to these clips so as to test the algorithms' resilience to different types of distortions in the input signal.

Three main categories of test clips were chosen, which are summarised below.

\subsubsection{Plain clips}

Plain clips are simply generated by cropping original song audio files to the required length, and are used as a first check during development to make sure the algorithm is functioning at a basic level. The match rate for these should be 100\%.

\subsubsection{Clips with added noise}

Clips with added noise are generated by taking plain clips and overlaying noise at different signal to noise ratios. The noise added might be generated, such as gaussian noise, or taken from a pre-recorded noise file. An example of pre-recorded noise might be a clip of ambient noise in a busy restaurant.

\subsubsection{Re-recorded clips}

Finally, a re-recorded clip is generated by playing a plain clip out of some speakers and re-recording the audio through a microphone. This captures more naturalistic distortions such as echo and reverb, as well as any inaccuracies, frequency biases or clipping introduced by the microphone or speakers.


\section{Software engineering decisions}

\subsection{Language choice}

One of my early focusses was deciding on the language to be used for the project, along with finding any libraries that would be helpful. My main considerations when making these decisions were ease of development, ease of evaluation, and my own familiarity.

To make development as easy as possible, I decided to make sure there was strong library support for operations which would be useful but could take a disproportionate amount of time to implement well myself, such as audio file manipulation. For example, a fast and accurate Discrete Fourier Transform implementation was essential.

I investigated several toolkits for music information retrieval, including in particular the C++ library OpenSMILE \cite{Eyben10} and the MATLAB library MIRToolbox \cite{Lartillot07}.

Ease of evaluation was also important. Although there wouldn't be much difference in writing test scripts in most programming languages, some languages, such as MATLAB, provide inbuilt support for plotting graphs, which could speed up my workflow significantly.

I was already somewhat familiar with MATLAB, having written a little for the Part 1A NST Mathematics course, and for various exercises in Part 1B. I was also familiar with C++ through the Part 1B course. However, I would not have described myself as experienced with either language.

The toolkits for both languages appeared to be roughly comparable for my needs. On balance, I decided that MATLAB was the better choice, mainly on the merit of its strong inbuilt library support for both graph output and common signal processing operations.


\subsection{Development strategy}
\label{section:devstrat}

Both of the algorithms to be implemented were well specified, so I did not expect any major changes in their overall structure or features during development, as might be expected when developing a commercial product or working with a client. 

As such, I deemed a waterfall model of development to be most appropriate, with the project having a clear linear path from requirements analysis to planning to implementation to evaluation. I decided to develop the two algorithms in series, with the Shazam algorithm coming first due to the fact I could find more sources of documentation for how it worked.

There was one exception to waterfall strategy, which was in the shared parts of the codebase used as a framework to run and test the two implementations. I expected some requirements of this part of the system to become more clear during development. As such, I developed the shared code in parallel with the first algorithm implementation, allowing the algorithm implementation to inform my implementation of the shared code. Some further refinements to the shared code were made during implementation of the second algorithm.


\subsection{Version control}

Git was used as my version control system during development and writeup. This provided a backup of all code via GitHub, as well as the ability to roll back changes and develop new features on separate branches. The commit history was also very useful as a supplement to my written notes when writing up the project.


\subsection{Backup strategy}

Development was carried out on my laptop. Regular full disk backups of the machine were taken using Time Machine onto an external drive, in case of corruption or malfunction. In addition, in case of theft or loss of both the laptop and hard drive, all data was backed up remotely. This was achieved using Google Drive for the music library, and a private GitHub repository for code and documentation.




\section{Technology familiarisation}

\subsection{MATLAB}

Since I had limited experience using MATLAB, particularly for larger programs, I had to re-familiarise myself with the language before beginning implementation. I did this mainly by working through small textbook-style exercises -- for example, implementing bubble sort -- and reading documentation for areas of the language I hadn't previously been exposed to such as namespacing, first-class functions and anonymous functions.

I created small relevant segments of code for things like reading in audio files and creating spectrograms of a signal in order to make sure I would comfortable implementing the project.

\subsection{MIRToolbox}

I also familiarised myself with MIRToolbox and its documentation. I started to investigate how useful it would be by importing some audio from an mp3 file and drawing a spectrogram of it. I implemented this task both with and without MIRToolbox, to compare the perfromance and ease of development. I found that the MIRToolbox version of the code was both slower and harder to develop, in that MIRToolbox tends to constrain you to a particular data flow to optimise certain operations, which makes it less flexible.

After consulting the operations available in the MATLAB standard library, I decided it would probably be possible to implement the algorithms just as easily without the use of MIRToolbox. Since, based on my preliminary tests, the MIRToolbox code might also be slower, I decided not to use MIRToolbox for my implementations.

\subsection{Database access}
\label{section:dbaccess}

Early on, I identified database access as a possible difficulty with my algorithm implementations. To ensure this would not be a problem, I decided to finish my familiarisation period by setting up a small database and making sure I could read and write to it.

Although MATLAB has an official Database Toolbox for database access, it is not included under the University's blanket license, so I investigated alternatives before attempting to obtain a license by other means.

I found an open source MATLAB SQLite3 driver \cite{Yamaguchi14} which worked well, and supported the full SQLite feature set, including batching multiple operations into transactions. After reading and writing to a small test database, I decided to use this database system for both my algorithm implementations.

%TODO? \section{Starting point}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}
\label{implementation}

In this section I will outline all of the steps I took to successfully implement the two selected audio fingerprinting algorithms. This starts with the design of the system (section \ref{section:systemdesign}), and then moves on to implementation of each of the Shazam and Philips algorithms (sections \ref{section:shazam} and \ref{section:philips} respectively).


\section{System design}
\label{section:systemdesign}

The first step of implementation was thorough planning in order to ensure a smooth development process. 

Audio fingerprinting naturally divides into two processes: building a database of song fingerprints, and matching unlabelled audio against this database. I will refer to the former as `registration', and the latter as `matching'.

Since matching and registration both involve calculating a fingerprint, the fingerprinting algorithm is best extracted into its own module, which can be called by both processes.

The code for actual registration, matching and fingerprinting is specific to each of the algorithms implemented, due to significant differences in their approaches to the problem. However, a significant amount of shared code was also required to feed data into the algorithms and collect results.

The design outlined for these two processes in the next two sections is based around a framework of shared code which allows different algorithm implementations to be swapped in in the relevant locations. In the diagrams, shared components are represented using blue blocks, and algorithm-specific components are represented in grey. 


\subsection{Registration}

The design for the registration process is summarised in Figure \ref{figs:registration_design}. 

\begin{figure}[h]
  \centering
  \input{figs/build_architecture}
  \caption{Block level design of the song registration system.}
  \label{figs:registration_design}
  \medskip \small
  The grey blocks within the matching section are algorithm-specific, while the blue blocks represent shared components.
\end{figure}

The reader module finds all the individual song files in the library folder, and passes them to the algorithm's song registrar.

The song registrar reads in the raw audio from the audio files, resamples to a standard sample rate and converts to mono. It then passes the signal to the fingerprinting module, which returns a fingerprint for the audio. The song registrar inserts the fingerprint into the algorithm's database, along with the song's metadata.

All of the grey components within the Registration process can be swapped out to allow for testing of both algorithms. This allows re-use of the reader code and makes for a cleaner codebase. %TODO capital R?



\subsection{Matching}


The design for the matching process is summarised in Figure \ref{figs:matching_design}. 

\begin{figure}[h]
  \centering
  \input{figs/test_architecture}
  \caption{Block level design of the testing system}
  \label{figs:matching_design}
  \medskip \small
  The grey blocks within the matching section are algorithm-specific, while the blue blocks represent shared components. The test producer can be swapped out to run different tests.
\end{figure}

The test producer begins with a procedure similar to the reader module in the registration process above, reading in song files one by one. However, this time, the files are taken from a subset of the library, randomly chosen in advance. This subset is the random sample used in each test. In practice, there are three such subsets and each test is run once on each subset, with the results averaged. This process is described in detail in the evaluation section.

After reading in each song to be tested, the test producer takes a clip of the song, and optionally applies some distortion to the clip. This distortion might range from adding background noise to playing the clip from speakers and re-recording it through a microphone.

These different distortion options are determined by passing the handle of a distortion function to the test producer, which applies that function to the audio signal.

The resulting test clip is passed by the tester to the algorithm's matching process, which creates a fingerprint from it and consults its database to try to find a match. The closest match is returned to the tester, and compared against the correct answer. Finally, the tester outputs the results of the tests.

In the same way as described for the registration process in the previous section, the grey components can be swapped out by swapping out function handles passed in to the tester.


\subsection{Component swapping}

Many components of the design need to be able to be swapped in and out depending on the algorithm or test type. 

In practice, this swapping-out is achieved by passing in function handles for swappable pieces of code. To use a different implementation, a different function handle can be passed.

For example, in the case of registration, the function handle for the song registrar is passed to the reader function, which calls it with the appropriate song files.


\section{Implementation of shared code}

This section details my implementation of the parts of the system shared by both algorithm implementations, which I will refer to as the `shared code'.

As outlined in the development strategy section (\ref{section:devstrat}), I decided to implement the shared code in parallel with the algorithm implementations.

Although my development strategy for the project as a whole used the waterfall model, my strategy for implementation of the shared code was closer to a spiral model, with iterated refinement of features and interface. Since the shared code needed to work with both algorithm implementations, its requirements were not totally clear from the start. 


\subsection{First prototype}

Without the shared code to feed in data, there is no way to test or run the algorithm implementations. In order to speed up my initial development and make sure I failed fast on any badly conceived paths, my first prototype of the shared code was simply the minimum needed to run the Shazam implementation. Getting this shared code working early was very useful in early debugging of the Shazam algorithm implementation.

The shared code consists of two parts, one for registration and one for matching. 

\subsubsection{Registration}

Since the registration half is relatively straightforward, the first prototype's implementation was actually almost complete. It consisted simply of a function which retrieved an array of every audio file in a given folder, and passed each filename to a registration function which is part of the algorithm implementation. It also outputs some progress information to the standard output so that when running the script I can monitor progress.

At the start of registering each song, each fingerprinting algorithm also needs to resample the audio to a fixed sample rate, and convert the signal from stereo to mono. However, each algorithm may require a different sample rate at this stage, and might require extra processing of some sort. As a result, I didn't include this preprocessing step in the shared code, instead I simply pass the audio file's location and let the algorithm's own registration function handle reading and resampling the audio.

In order to avoid duplication of code, I wrote a utility function to resample audio to a mono signal at a given sample rate, which was straightforward with the Signal Processing Toolbox.

\subsubsection{Matching}

The matching half is more complicated since it includes all of the testing functionality. The first prototype's version was severely limited -- it simply took audio files from a tests folder and ran them against the algorithm's matching function. The tests in this folder were clips from original songs, which I manually cropped from the original files using the free software Audacity. It then printed the returned match result to the MATLAB command line, which I could manually verify.

This testing process was used for very early debugging of the Shazam algorithm implementation but quickly became too laborious for real evaluation.


\subsection{Full implementation}

Once an implementation of the Shazam algorithm was completed, and the prototype shared code was used as a first check that it was working, more features were needed from the shared code to carry out further evaluation that the implementation was working as intended.

The prototype was expanded to include all of the features described in the system design section (\ref{section:systemdesign}). As well as this, I reviewed the code to make sure it was well structured, consistently styled and generic enough to allow use with the second algorithm. 

Where I identified issues, I refactored the code. This refactoring step was important for maintainability of the code, since rapid development of the first prototype as well as my lack of experience with MATLAB at the start of the project had left behind badly structured and inconsistently styled code in a few areas.


\subsubsection{Registration}

Upgrading the registration process for the full implementation was relatively fast, since most of the functionality was already finished in the prototype phase. However, some refactoring and modification was still required.

For example, the first prototype did not actually support different registration functions -- it was hard coded to call the Shazam algorithm's registration function. Obviously, this would not be sufficient for the full project, and I refactored the code to take a function handle for the registration function to make it more generic.

In addition, I enabled the registration process to call two more functions, one at the start of the process and another at the end. The purpose of these was to allow algorithms to run arbitrary code at the start and end of the process, to allow them to do things like initialise the database and clean it up at the end. This was most mostly needed for an optimisation involving the database indexes, which is discussed in section \ref{section:shazamoptimisations}.

\subsubsection{Matching}

The matching process was missing more features in the prototype than registration.

First to be added was the test producer (shown in context in Figure \ref{figs:matching_design} on page \pageref{figs:matching_design}). Whilst the prototype required hand-made test clips, the test producer creates them automatically as required from the original library.

To create test clips, the test producer first selects $n$ random songs from the library (where $n$ is specified as part of the test parameters). The test producer takes a clip from each of the songs, and saves the clip to a new folder of test clips. Before saving the clip back, it can optionally apply some distortion to the clip. These distortions, and their development, are discussed further in the evaluation section. % TODO reference

The test producer finally passes a list of the test clips it has created to the next step, the tester, which passes each clip to the algorithm's matching function in the same way as described for the prototype.

One important change made to the tester was to enable it to automatically classify matches as correct or incorrect, by tagging each clip coming out of the test producer with its corresponding correct answer. Test results could then be output directly by the tester as a proportion of correct matches.


\subsection{Final refinements}

The shared code detailed in the section above was good enough to verify that the Shazam algorithm was working well and could match undistorted clips with near 100\% accuracy.

A few further improvements were made during my implementation of the Philips algorithm. The primary improvement was to allow the matching process to use test sets -- that is, instead of the test producer choosing $n$ random songs each time, random subsets of the library are defined in advance as test sets, and all songs in a test set are used to produce test clips.

I wrote a new script, which produced multiple subsets of the library to draw tests from. It did this by copying songs to new test set folders, and only needed to be run once for each test set to be created. 

The test producer was modified to produce test clips from all songs in a given test set folder. I also added the functionality to run batches of tests from multiple test sets. This testing functionality is further discussed in the evaluation section. %TODO reference



\section{Implementation of the Shazam algorithm}
\label{section:shazam}

This section is a summary of my work in implementing the first of the two fingerprinting algorithms, which was proposed by Avery Wang in 2003 \cite{Wang03}, and which I will refer to as the `Shazam algorithm'. Section \ref{section:algoselection} contains more discussion on the rationale used to select the algorithms.

The original paper describes the algorithm as `capable of quickly identifying a short segment of music captured through a cellphone microphone in the presence of foreground voices and other dominant noise.' %TODO quote needed here?

I will describe development over the next four subsections in terms of the four main components of the algorithm. These correspond to the components discussed in section \ref{section:systemdesign}, namely: 

\begin{itemize}
  \item A \textbf{fingerprinting function} to calculate reproducible hash tokens from audio
  \item A \textbf{database} to store fingerprints for known songs
  \item A \textbf{registrar} to add fingerprints to the database
  \item A \textbf{matcher} to match unlabelled audio to labelled fingerprints in the database
\end{itemize}

\subsection{Fingerprinting function}

In order to identify signals in a robust manner, features of the audio which are relatively invariant in the face of noise and distortions must be used as a basis for fingerprinting.

\subsubsection{Constellation maps}

The fingerprinting function of the Shazam algorithm uses spectrogram peaks as its chosen features. A time-frequency point is chosen as a feature if it is a local energy maxima -- that is, it has more energy than any of its close neighbors.

In this way, a complex spectrogram can be reduced to a sparse set of time-frequency coordinates, which are relatively robust to signal degradations. The paper terms this list of coordinates a `constellation map', since it can resemble a star field. 

The pattern of dots in the constellation map for two matching pieces of audio should be the same: if you slide copies of each map over each other, there should be some time offset where all of the points overlap. In practice, not all of the points will match because distortions may have introduced or deleted points.

So far, our fingerprinting algorithm consists of computing a spectrogram for the input audio, finding peaks in it, and then storing these peaks to be matched against later. Matching becomes a process of trying to find the patch of stars in our database of constellation maps which most closely matches the small patch of stars we have calculated for our input clip. This process is discussed more in section \ref{shazam:matcher} but involves, for each peak in our input audio's constellation map, finding all peaks at the same frequency in the database. I will call these possible matches `candidate matches'.

\subsubsection{Combinatorial hashing}

Unfortunately, finding matches in this way can be slow due to the low entropy of the constellation points. Using 1024 frequency bins results in a maximum of 10 bits of entropy which can be used to find candidate matches in the database. 

In order to increase this entropy and thus decrease the false match rate for candidate matches, Wang uses a combinatorial hashing technique. Each peak is paired with a number of other points in a target zone. The frequencies of both peaks and the time offset between them is packed into a 32-bit unsigned integer, which is returned as a hash value. Each hash value is also associated with the time offset from the start of the audio signal, although this does not make up part of the hash itself.

By again using 1024 possible frequency values, and another 10 bits for the time offset between the two peaks, we now gain 30 bits of information as opposed to our previous 10. The 20 additional bits give each hash around a million times more specificity, and a similar search speedup. However, there is also a combinatorial explosion of the number of hashes. In order to limit this, a maximum fan-out factor $F$ is defined, and only the first $F$ peaks in the target zone are paired to form hashes. For $F=10$, as is used in my implementation, the end result is a speedup factor in the region of $10^5$ over the na{\"i}ve strategy described above.

\subsubsection{Implementation}

Computing the spectrogram of the audio was achieved using the \lstinline{spectrogram} function in the MATLAB Signal Processing Toolbox.

The second phase, finding the local maxima, is done in three passes. The first pass is a simple 3 by 3 median filter, which reduces salt and pepper noise. This ensures peaks with very small amplitudes, which are much more distortion sensitive, are eliminated. 

The second pass is a gaussian filter which smooths the spectrogram, removing high frequency components of the time-frequency signal. Again, this removes some high frequency noise and also makes sure that our peaks have a reasonable spacing; peaks too close together are merged. The size of this gaussian filter determines the feature size of the peaks we detect. 

The final step of peak finding is to compare each point with its neighbours. If it is the largest of all its 8 neighbours, it is classified as a peak.

As described above, the peaks in the constellation map are then paired and each pair is packed into a 32 bit integer hash. The list of hashes is then returned, along with their associated time offsets.


\subsection{Database}

As I discuss in section \ref{section:dbaccess}, the fingerprint databases use SQLite3. The database structure is straightforward, with a \lstinline{songs} table to store song metadata and a \lstinline{hashes} table to store fingerprint data.

The \lstinline{songs} table has two columns, \lstinline{song_id} and \lstinline{song_name}, and acts as a list of all songs in the database, along with an identifier for each one. In a real world implementation this table would almost certainly contain further metadata for each song, such as the artist, record label and album name. This extra information is not relevant to the algorithm itself and would have taken extra time to collect, so I omitted it.

The \lstinline{hashes} table has three columns: \lstinline{song_id}, \lstinline{hash}, and \lstinline{time}. The \lstinline{hash} column in each row stores frequency and time difference information for one pair of peaks, encoded as a 32-bit unsigned integer. The \lstinline{song_id} and \lstinline{time} columns store the identifier of the song the peaks are from, and time offset (in samples) from the beginning of the song.


\subsection{Registrar}
\label{shazam:registrar}

The registrar is responsible for registering a new song in the database. It consists of a single function, which receives an audio filename and a handle to the database. 

It reads in the audio from the file, and resamples it to an 8kHz mono signal as a preprocessing step. It then calls the fingerprinter to obtain a fingerprint of the signal, and inserts the fingerprint hashes into the database, along with the song's name. Note that, for simplicity, the song's name is taken to be its filename.


\subsection{Matcher}
\label{shazam:matcher}

The matching process is the final component of the system. It receives unlabelled audio and finds the closest matching song in the database. The implementation is described below in terms of the four main steps needed to match a clip of unlabelled audio.

\subsubsection{Fingerprinting of unlabelled audio}

The first step is to obtain the audio signal, and to calculate its fingerprint. In the case of my tests, the unlabelled audio signal is presented as a small audio file, but in a real system it might come directly from a microphone. The fingerprint is calculated by means of a call to the fingerprinting module.

\subsubsection{Finding candidate songs}

The next step is to find candidate songs in the database which might match our unlabelled audio. To do this, we simply take all of the hashes -- time-frequency peak pairs -- in the unlabelled audio's fingerprint, and search for identical hashes in the database. This search is made efficient by means of an index on the \lstinline{hash} column of the database.

Each of these matches is binned according to the \lstinline{song_id}. This results in a list of candidate songs, each with a corresponding list of matching hashes. Each matching hash is associated with two time offsets: one  from the start of the song in the database, and one from the start of the unlabelled audio.

\subsubsection{Scoring candidate songs}

Having found a list of candidate songs, we now need to score each in order to find out which one is the closest match.

The list of hash matches corresponding to a candidate song represents a scatterplot, with time in the candidate song along one axis and time in the unlabelled audio along the other. If two files matched, we would expect a sequence of features in one file to also occur in the same relative time sequence (minus some offset) in the other file. That is, we should expect a diagonal line in the scatterplot of matching hashes.

A robust regression technique could be used to detect this line, but would be computationally expensive. To constrain the problem, we assume that the gradient of the line is 1. This means we lose the ability to detect clips played back faster or slower than the original song, but in practice this is a rare distortion.

Using this assumption, the corresponding times of matching features between matching signals have the relationship

\begin{align*}
  t_f' = t_f + k 
\end{align*}

where $t_f$ is the time of a feature $f$ in the unlabelled audio clip, and $t_f'$ is the time of the same feature in the candidate song. $k$ is the time offset between the sequences (the start time of the clip in the original song).

For each $(t_f',t_f)$ pair in the scatterplot, we calculate

\begin{align*}
  \delta t_f = t_f' - t_f
\end{align*}

Then, we calculate a histogram of these $\delta t_f$ values. If a diagonal line exists in the scatterplot, we will get a sharp peak in the histogram at the associated offset (where $\delta t_f = k$).

The score assigned to each candidate song is simply the height of the highest peak in this histogram.


\subsubsection{Results}

The final task is to sort the candidate songs in descending order of score and return the top match. 

My implementation simply returns the candidate with the highest score. A more sophisticated implementation might attempt to assess the probability of a false match given the score of the top candidate, and return no match if the significance of the score is too low. However, this requires statistical analysis of the song library in question to determine false positive rates for different scores.

For the purposes of this project I decided that adding this feature for both implementations would require too much time, and would not contribute enough to the evaluation of the algorithms.


\subsection{Optimisations}
\label{section:shazamoptimisations}

\subsubsection{Database transactions}
\subsubsection{Database indexes}
\subsubsection{Memory allocation}


\section{Implementation of the Philips algorithm}
\label{section:philips}

\subsection{Fingerprinting function}
\subsection{Database}
\subsection{Registrar}

The Philips algorithm's registration process works identically to the Shazam algorithm's counterpart, which is described in section \ref{shazam:registrar}.

\subsection{Matcher}
\subsection{Optimisations}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Evaluation}
\label{evaluation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}
\label{conclusion}

In this project I successfully implemented two algorithms for audio fingerprinting and demonstrated their use for song recognition. I gathered a library of songs to test against, and used it to evaluate and compare both algorithms against a variety of criteria.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix


\chapter{Project Proposal}

\input{proposal}

\end{document}
