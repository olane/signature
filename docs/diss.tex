
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage[UKenglish]{babel}

% Set distance between figures and text
\setlength\floatsep{2\baselineskip}
\setlength\textfloatsep{2\baselineskip}
\setlength\intextsep{2\baselineskip}

% TikZ is used for diagrams
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,fit}

% Matlab-prettifier is used for matlab code formatting
\usepackage[numbered,framed]{matlab-prettifier}
\lstset{
  style              = Matlab-editor,
  basicstyle         = \mlttfamily,
  mlshowsectionrules = true,
}

\captionsetup{labelfont=bf} % Make figure labels bold


\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable


\parindent 0pt % Disable paragraph indent and add paragraph spacing
\parskip 6pt

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Oliver Lane}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Audio Fingerprinting for Music Recognition} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity Hall \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preamble

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Oliver Lane                       \\
College:            & \bf Trinity Hall                     \\
Project Title:      & \bf Audio Fingerprinting for Music Recognition \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2015  \\
Word count:         & \bf 11,677  \\
Project Originator: & Oliver Lane                    \\
Supervisor:         & Vaiva Imbrasait\'{e}                    \\ 
\end{tabular}
}

\section*{Original aims of the project}

In this project I aimed firstly to implement at least two audio fingerprinting algorithms, each with a corresponding matching algorithm, for the purpose of recognising clips of songs from a library in a way that is robust to noise and distortions.

Secondly, I aimed to assemble a library of songs and corresponding test clips against which to test these algorithms.

Finally, I aimed to compare these implementations against several criteria, including size of fingerprints generated and percentage of test clips matched correctly from the assembled library, at various clip lengths and levels of noise.

\section*{Work completed}

Two algorithms for audio fingerprinting were successfully implemented, with corresponding matching algorithms. These algorithms were proposed by Avery Wang in 2003 \cite{Wang03} and Haitsma et al. in 2002 \cite{Haitsma02}. A library of songs was assembled, and corresponding test cases made by taking different length clips from the library and adding distortions.

The two implementations were tested and evaluated against several criteria using the set of test clips, and compared against each other.

\section*{Special difficulties}

None.

 
\newpage
\section*{Declaration of originality}

I, Oliver Lane of Trinity Hall, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in  it are my own work, unaided except as may be specified below, and that the  dissertation does not contain material that has already been used to any substantial extent for a comparable purpose.

\bigskip
\bigskip
\bigskip

\leftline{Signed}

\bigskip
\bigskip

\leftline{Date}

\newpage
\section*{Acknowledgements}

My sincere thanks go to my Supervisor Vaiva Imbrasait\'{e} and my Director of Studies Professor Simon Moore for support and guidance throughout the project.

I would also like to thank my neighbours for tolerating hours of short music clips coming through their walls, as well as my friends for many pots of tea.

Finally, thanks to my parents for always encouraging me to do what I enjoy, and to Giles Lavelle for proof reading some downright diabolical tense switches.


\tableofcontents

%\listoffigures


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              CHAPTERS

\pagestyle{headings}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{introduction}

This project concerns the problem of identifying songs from small fragments of audio. Algorithms used to tackle this problem are usually referred to as \emph{audio fingerprinting} algorithms. In this project, I successfully implement two such algorithms, and compare their relative strengths and weaknesses.

This section serves as an introduction to the subject area, and summarises the main content of the project.

\section{Introduction to audio fingerprinting}

Audio fingerprinting gives the ability to identify metadata for songs, videos and other media based on the content of the audio signal (or, often, a small section of that audio).

This technology has found a variety of uses in both industry and consumer products. For example, a user might want to identify a song's name and artist by recording a clip of it using their mobile phone. Music labels might use the technology to automatically monitor radio stations to ensure the correct song royalties are being paid, or to use as part of a watermarking system to track copyright violations.


\subsection{Fingerprinting functions}

An audio fingerprint is a compact digital representation of some audio, which summarises its content. It is generated deterministically from the audio signal, and can be thought of as a kind of hash, where the aim is that perceptually similar pieces of audio have similar hash values. Perceptual similarity is used here to mean that two clips sound similar to the human ear, although this is somewhat difficult to quantify.

An audio fingerprinting algorithm needs to work independently to the representation of the audio signal. If two audio signals sound the same to the human ear then their fingerprints should be a close match, regardless of how similar the binary representation of the audio was. For instance, two copies of the same song encoded at different bitrates using a lossy compression scheme such as MP3  will have quite different representations but are very perceptually similar, and so should have very similar fingerprints.

As a result, a robust audio fingerprinting algorithm needs to take into account the perceptual characteristics of the audio, and be as immune as possible to likely distortions. Despite this, it must retain enough discriminatory power to distinguish between similar songs.

Once an audio fingerprint has been computed for a fragment of audio, a precomputed database of song fingerprints can be searched for a close match. Here, a close match is defined by some distance metric which determines the similarity between two fingerprints. If a close match is found, the algorithm concludes that the fragment of audio matches the song found in the database.

Note that it is not possible to create a fingerprint function which always results in identical fingerprints for perceptually similar audio, because perceptual similarity is known not to be transitive. That is, if an audio clip A is perceptually similar to two other clips, B and C, then B and C are not necessarily perceptually similar. A fingerprinting scheme which relied on absolute equality would enforce such a property, and would therefore be unsuitable for modelling perceptual similarity.

In conclusion, when designing a fingerprinting function, it is required that two perceptually similar audio objects result in similar fingerprints. In addition, two dissimilar audio objects should be very unlikely to produce similar fingerprints. 

More formally, a well designed fingerprint function $F$ and corresponding distance function $D$ will have some threshold $t$ such that with very high probability $D(F(A),F(B)) <= t$ if objects A and B are perceptually similar, and $D(F(A),F(B)) > t$ if they are not.


\subsection{General framework}

Despite their differences in approach, most audio fingerprinting algorithms share a common framework.

Each algorithm consists of three main parts: 

\begin{itemize}
  \item A fingerprinter, which generates an audio fingerprint from a given piece of audio.
  \item A registrar, which builds a database of fingerprints from a library of song audio files and their metadata.
  \item A matcher, which matches a fingerprint against the precomputed database.
\end{itemize}

This structure is shown in more detail in Figure \ref{fig:generalframework}. 

\begin{figure}[h]
  \centering
  \input{figs/general_framework}
  \caption{General structure of an audio fingerprinting algorithm}
  \label{fig:generalframework}
\end{figure}

Given that the matcher needs to search the database for close matches, some notion of distance is needed to decide how similar two fingerprints are. A library can grow to many thousands of songs in size, and distance comparisons can be computationally expensive, so methods of speeding up the database search are often employed. This often involves using a cheaper distance measure first to discard unlikely candidates, before using a more accurate measure to hone in on matches.


\subsection{System parameters}
\label{section:systemparams}

Audio fingerprinting systems can be evaluated over a variety of parameters, which vary in importance based on the application. The main parameters are summarised below.

\subsubsection{Robustness}

Robustness to signal degradations is one of the most important parameters. Ideally, even severely degraded audio should give a very similar fingerprint. All algorithms require some robustness to resampling and mild compression, since there is no guarantee that the input audio will be in the same format and coded at the same compression rate as the original copy in the library.

Most algorithms will also try to be as robust as possible to other distortions such as echo, reverb, severe compression, background noise and equalisation.

In order to remain robust to distortions, algorithms must base their fingerprints on features of the audio which are reasonably invariant to the distortions in question. Usually these are perceptual features such as frequency peaks.

\subsubsection{Reliability}

Reliability has to do with the proportion of songs which are incorrectly matched. The rate at which this occurs is usually referred to as the false positive rate or the false match rate.

This would be slightly more important, for example, to a music label tracking royalties than to a user finding song titles using their smartphone. However, this metric is always fairly important.

\subsubsection{Fingerprint size}

The size of the fingerprint calculated is important, because to identify metadata a database of song fingerprints must be created. The larger each fingerprint is in size, the faster this database will grow.

\subsubsection{Granularity}

The property of granularity determines how many seconds of audio are required to identify a match in the database. In some applications the whole song can be used for identification, in others shorter clips are preferred. 

The granularity will often depend on the types of degradation applied to the audio; many algorithms can compensate for more severely degraded audio by matching on longer clips.

\subsubsection{Scalability}

Scalability refers to how easily the system can be extended for larger and larger library sizes. A larger library means a larger fingerprint database, and therefore longer search times. Efficient database search strategies are usually employed to keep search times as low as possible.


\section{This project}

There are a many algorithms for audio fingerprinting which have been proposed in the literature, some of which have been used extensively for commercial applications. 

Two such algorithms were selected and implemented for the project.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preparation}
\label{preparation}

The next three sections will cover all of the work done for the project. This section will summarise the work that was done before implementation of any code began. 

Broadly, this encompasses research on the literature around the topic, learning of new skills to be used during implementation, and planning of the implementation and evaluation portions of the project. It also involved the selection of two algorithms to be implemented and compared.

Sections \ref{implementation} and \ref{evaluation} will cover the actual implementation of the two algorithms selected, and the subsequent evaluation of those implementations against each other.


\section{Review of audio fingerprinting techniques}

The first step undertaken was to review the current literature around the area.

A focus of mine during this review was to understand what my system design might look like, given that the design had to be generic enough to allow the implementation of at least two algorithms with different approaches. 

Reading individual papers for proposed algorithms was useful in this regard, but it was often difficult to separate algorithm-specific features from general features of audio fingerprinting systems. One particularly useful paper in developing this understanding is due to Cano et al. \cite{Cano02}, which reviews many audio fingerprinting algorithms and provides a good overview of the general structure of most audio fingerprinting algorithms. 

This general overview combined with basic understanding of a few specific algorithms gave me a good starting point for my system design.


\section{Algorithm selection}
\label{section:algoselection}

In selecting which algorithms to compare, I wanted two algorithms which were:

\begin{itemize}
  \item Reasonably similar in their aims, so that they could be compared fairly
  \item Reasonably different in their approach, so that there was still something to compare
\end{itemize}

There are many parameters affecting the design and performance of an audio fingerprinting system, which are described in more detail in section \ref{section:systemparams}. One of the most important, and most studied, is how robust the system is to degradation of input audio through various distortions. 

The specific distortions that an algorithm needs to be robust towards are somewhat dependent on the intended use cases. For example, a system intended for radio broadcast monitoring might need to be robust against radio noise or compression, but wouldn't need to worry about reverb or matching using very short clips. However, a system intended to match songs from a smartphone app would require robustness to background noise (such as ambient crowd noise) and reverb, and would benefit from maintaining high accuracy even when only very short clips of the full track are available.

The latter use case imposes more constraints on the problem in terms of severity of distortion, especially given the commodity hardware involved. As such, it is generally more interesting from a signal processing perspective, and both algorithms selected for this project try to tackle this use case.

The first algorithm I selected was an algorithm developed by Avery Wang in 2003 and deployed in the smartphone app Shazam \cite{Wang03}. I chose this as a good algorithm to implement first, since it is well documented both in the literature and on several blogs and forums. I will refer to this as the \textit{Shazam algorithm}.

The second algorithm selected was an algorithm proposed by Philips researchers Haitsma et al. in 2002 \cite{Haitsma02}. I will refer to this as the \textit{Philips algorithm}. Although both algorithms begin with a similar step of taking the Fourier transform of the audio, the Shazam algorithm extracts local frequency-time domain maxima as its features, whilst the Philips algorithm computes a calculation based on the energy in different frequency bands. I decided that this pair represented a reasonable choice with respect to the two aims stated above.


\section{Evaluation}

\subsection{Library}

To test the audio fingerprinting algorithms, a reasonably large library of songs was needed to match clips against. Both of the papers I focused on tested their algorithms on databases of 10,000 songs, but a library this size would be difficult for me to gather myself. Since I could find no test sets available for this sort of evaluation, I decided to assemble my own library. A library size of at least 500 songs was decided, as a compromise between time constraints and the quality of the testing.

Genre and style diversity in the song library was also an important consideration, in order to reduce biases in the testing process. My own local music library was used as the basis for my test set (around 600 songs). Fortunately my library is fairly diverse, but I also supplemented the selection with more songs from the Free Music Archive to increase the representation of genres less common in my local collection. The full test library consists of 720 songs.

\subsection{Test strategy}

I decided on a testing strategy, which consists of inputting clips of known songs from the library into the algorithms, and recording whether each clip was matched to the correct song. Different distortions are applied to these clips so as to test the algorithms' resilience to different types of distortions in the input signal.

I also created a set of plain clips, generated by simply cropping original song audio files to the required length. These are useful as a first check during development, to ensure the algorithm is functioning at a basic level. The match rate for these plain clips should be 100\%.


\section{Software engineering decisions}

\subsection{Language choice}

One of my early focuses was deciding on the language to be used for the project, along with finding any libraries that would be helpful. My main considerations when making these decisions were ease of development, ease of evaluation, and my own familiarity.

To make development as easy as possible, I decided to make sure there was strong library support for operations which would be useful but could take a disproportionate amount of time to implement well myself, such as audio file manipulation. For example, a fast and accurate Discrete Fourier Transform implementation was essential.

I investigated several toolkits for music information retrieval, including in particular the C++ library OpenSMILE \cite{Eyben10} and the MATLAB library MIRToolbox \cite{Lartillot07}.

Ease of evaluation was also important. Although there wouldn't be much difference in writing test scripts in most programming languages, some languages, such as MATLAB, provide inbuilt support for plotting graphs, which could speed up my workflow significantly.

I was already somewhat familiar with MATLAB, having written a little for the Part 1A NST Mathematics course, and for various exercises in Part 1B. I was also familiar with C++ through the Part 1B course. However, I would not have described myself as experienced with either language.

The toolkits for both languages appeared to be roughly comparable for my needs. On balance, I decided that MATLAB was the better choice, mainly on the merit of its strong inbuilt library support for both graph output and common signal processing operations.


\subsection{Development strategy}
\label{section:devstrat}

Both of the algorithms to be implemented were well specified, so I did not expect any major changes in their overall structure or features during development, as might be expected when developing a commercial product or working with a client. 

As such, I deemed a waterfall model of development to be most appropriate, with the project having a clear linear path from requirements analysis to planning to implementation to evaluation. I decided to develop the two algorithms in series, with the Shazam algorithm coming first due to the fact I could find more sources of documentation for how it worked.

There was one exception to waterfall strategy, which was in the shared parts of the codebase used as a framework to run and test the two implementations. I expected some requirements of this part of the system to become more clear during development. As such, I developed the shared code in parallel with the first algorithm implementation, allowing the algorithm implementation to inform my implementation of the shared code. Some further refinements to the shared code were made during implementation of the second algorithm.


\subsection{Version control}

Git was used as my version control system during development and writeup. This provided a backup of all code via GitHub, as well as the ability to roll back changes and develop new features on separate branches. The commit history was also very useful as a supplement to my written notes when writing up the project.


\subsection{Backup strategy}

Development was carried out on my laptop. Regular full disk backups of the machine were taken using Time Machine onto an external drive, in case of corruption or malfunction. In addition, in case of theft or loss of both the laptop and hard drive, all data was backed up remotely. This was achieved using Google Drive for the music library, and a private GitHub repository for code and documentation.




\section{Technology familiarisation}

\subsection{MATLAB}

Since I had limited experience using MATLAB, particularly for larger programs, I had to re-familiarise myself with the language before starting the implementation. I did this mainly by working through small textbook-style exercises -- for example, implementing bubble sort -- and reading documentation for areas of the language I hadn't previously been exposed to such as namespacing, first-class functions and anonymous functions.

I created small relevant segments of code for things like reading in audio files and creating spectrograms of a signal in order to make sure I would comfortable implementing the project.

\subsection{MIRToolbox}

I also familiarised myself with MIRToolbox and its documentation. I started to investigate how useful it would be by importing some audio from an MP3 file and drawing a spectrogram of it. I implemented this task both with and without MIRToolbox, to compare the performance and ease of development. I found that the MIRToolbox version of the code was both slower and harder to develop, in that MIRToolbox tends to constrain you to a particular data flow to optimise certain operations, which makes it less flexible.

After consulting the operations available in the MATLAB standard library, I decided it would probably be possible to implement the algorithms just as easily without the use of MIRToolbox. Since, based on my preliminary tests, the MIRToolbox code might also be slower, I decided not to use MIRToolbox for my implementations.

\subsection{Database access}
\label{section:dbaccess}

Early on, I identified database access as a possible difficulty with my algorithm implementations. To ensure this would not be a problem, I decided to finish my familiarisation period by setting up a small database and making sure I could read and write to it.

Although MATLAB has an official Database Toolbox for database access, it is not included under the University's blanket license, so I investigated alternatives before attempting to obtain a license by other means.

I found an open source MATLAB SQLite3 driver \cite{Yamaguchi14} which worked well, and supported the full SQLite feature set, including batching multiple operations into transactions. After reading and writing to a small test database, I decided to use this database system for both my algorithm implementations.

%TODO? \section{Starting point}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}
\label{implementation}

In this section I outline all of the steps I took to successfully implement the two selected audio fingerprinting algorithms. This starts with the design of the system (section \ref{section:systemdesign}), and then moves on to implementation of each of the Shazam and Philips algorithms (sections \ref{section:shazam} and \ref{section:philips} respectively).


\section{System design}
\label{section:systemdesign}

The first step of implementation was thorough planning in order to ensure a smooth development process. 

Audio fingerprinting naturally divides into two processes: building a database of song fingerprints, and matching unlabelled audio against this database. I will refer to the former as `registration', and the latter as `matching'.

Since matching and registration both involve calculating a fingerprint, the fingerprinting algorithm is best extracted into its own module which can then be called by both processes.

The code for registration, matching and fingerprinting is specific to each of the algorithms implemented, due to large differences in their approaches to the problem. However, a significant amount of shared code was also required to feed data into the algorithms and collect results.

The design outlined for these processes in the next two sections is based around a framework of shared code which allows, in the relevant locations, different algorithm implementations to be swapped in. In the diagrams, shared components are represented using blue blocks, and algorithm-specific components are represented in grey. 


\subsection{Registration}

\begin{figure}[htb]
  \centering
  \input{figs/build_architecture}
  \caption{Block level design of the song registration system.}
  \label{figs:registration_design}
  \medskip \small
  The grey blocks within the matching section are algorithm-specific, while the blue blocks represent shared components.
\end{figure}

The design for the registration process is summarised in Figure \ref{figs:registration_design}. 

The reader module finds all the individual song files in the library folder, and passes them to the algorithm's song registrar.

The song registrar reads in the raw audio from the audio files, resamples to a standard sample rate and converts to a monophonic signal. It then passes the signal to the fingerprinting module, which returns a fingerprint for the audio. The song registrar inserts the fingerprint into the algorithm's database, along with the song's metadata.

All of the grey components within the registration process can be swapped out to allow for testing of both algorithms. This allows reuse of the reader code and makes for a cleaner codebase.


\subsection{Matching}

\begin{figure}[htb]
  \centering
  \input{figs/test_architecture}
  \caption{Block level design of the testing system}
  \label{figs:matching_design}
  \medskip \small
  The grey blocks within the matching section are algorithm-specific, while the blue blocks represent shared components. The test producer can be swapped out to run different tests.
\end{figure}

The design for the matching process is summarised in Figure \ref{figs:matching_design}.

The test producer begins with a procedure similar to the reader module in the registration process above, reading in song files one by one. However, this time, the files are taken from a subset of the library, randomly chosen in advance. This subset is the random sample used in each test. In practice, there are three such subsets and each test is run once on each subset, with the results averaged. This process is described in detail in the evaluation (section \ref{evaluation:matching_with_distortion}).

After reading in each song to be tested, the test producer takes a clip of the song, and optionally applies some distortion to the clip. This distortion might range from adding background noise to playing the clip from speakers and re-recording it through a microphone.

These different distortion options are determined by passing the handle of a distortion function to the test producer, which applies that function to the audio signal.

The resulting test clip is passed by the tester to the algorithm's matching process, which creates a fingerprint from it and consults its database to try to find a match. The closest match is returned to the tester, and compared against the correct answer. Finally, the tester outputs the results of the tests.

In the same way as described for the registration process in the previous section, the grey components can be swapped out depending on the algorithm.


\subsection{Component swapping}

Many components of the design need to be able to be swapped in and out depending on the algorithm or test type. 

In practice, this swapping-out is achieved by creating function handles\footnote{A \textit{function handle} is MATLAB terminology for a first-class function.} for swappable pieces of code. To use a different implementation, a different function handle can be passed to the code that calls that function.

For example, in the case of registration, the function handle for the song registrar is passed to the reader function, which calls it with the appropriate song files.


\section{Implementation of shared code}
\label{section:sharedimpl}

This section details my implementation of the parts of the system shared by both algorithm implementations, which I will refer to as the `shared code'.

As outlined in the development strategy section (\ref{section:devstrat}), I decided to implement the shared code in parallel with the algorithm implementations.

Although my development strategy for the project as a whole used a waterfall model, my strategy for implementation of the shared code was closer to a spiral model, with iterated refinement of features and interface. This was useful since the shared code needed to work with both algorithm implementations, and so its requirements were not totally clear from the start. 


\subsection{First prototype}

Without the shared code to feed in data, there is no way to test or run the algorithm implementations. In order to speed up my initial development and make sure I failed fast on any badly conceived paths, my first prototype of the shared code was simply the minimum needed to run the Shazam implementation. Getting this shared code working early was very useful in early debugging of the Shazam algorithm implementation.

The shared code consists of two parts, one for registration and one for matching. 

\subsubsection{Registration}

Since the registration half is relatively straightforward, the first prototype's implementation was actually almost complete. It consisted simply of a function which retrieved an array of every audio file in a given folder, and passed each filename to a registration function which is part of the algorithm implementation. It also outputted some progress information to the standard output so that when running the script I could monitor progress.

At the start of registering each song, each fingerprinting algorithm also needs to resample the audio to a fixed sample rate, and convert the signal from stereo to a single channel. However, each algorithm may require a different sample rate at this stage, and might require extra processing of some sort. As a result, I didn't include this preprocessing step in the shared code, instead I simply passed the audio file's location and let the algorithm's own registration function handle reading and resampling the audio.

In order to avoid duplication of code, I wrote a utility function to resample audio to a monophonic signal at a given sample rate, which was straightforward with the MATLAB Signal Processing Toolbox.

\subsubsection{Matching}

The matching half is more complicated since it includes all of the testing functionality. The first prototype's version was severely limited -- it simply took audio files from a tests folder and ran them against the algorithm's matching function. The tests in this folder were clips from original songs, which I manually cropped from the original files using the free software Audacity \cite{Audacity}. It then printed the returned match result to the MATLAB command line, which I could manually verify.

This testing process was used for very early debugging of the Shazam algorithm implementation but quickly became too laborious for real evaluation.


\subsection{Full implementation}

Once an implementation of the Shazam algorithm was completed, and the prototype shared code was used as a first check that it was working, more features were needed from the shared code to carry out further evaluation that the implementation was working as intended.

The prototype was expanded to include all of the features described in the system design section (\ref{section:systemdesign}). As well as this, I reviewed the code to ensure it was well structured, consistently styled and generic enough to allow use with the second algorithm. 

Where I identified issues, I refactored the code. This refactoring step was important for maintainability of the code, since rapid development of the first prototype as well as my lack of experience with MATLAB at the start of the project had left behind badly structured and inconsistently styled code in a few areas. These problem areas were restructured and fixed, leaving a much more manageable codebase.


\subsubsection{Registration}

Upgrading the registration process for the full implementation was relatively fast, since most of the functionality was already finished in the prototype phase. However, some refactoring and modification was still required.

For example, the first prototype did not actually support different registration functions -- it was hard coded to call the Shazam algorithm's registration function. Obviously, this would not be sufficient for the full project, and so I refactored the code to take a function handle for the registration function, making it more generic.

In addition, I enabled the registration process to call two more functions, one at the start of the process and another at the end. The purpose of these was to allow algorithms to run arbitrary code at the start and end of the process, to allow them to do things like initialise the database and clean it up at the end. This was most mostly needed for an optimisation involving the database indexes, which is discussed in section \ref{section:shazamoptimisations}.

\subsubsection{Matching}

The matching process was missing more features in the prototype than registration.

First to be added was the test producer (shown in context in Figure \ref{figs:matching_design} on page \pageref{figs:matching_design}). Whilst the prototype required hand-made test clips, the test producer creates them automatically as required from the original library.

To create test clips, the test producer first selects $n$ random songs from the library (where $n$ is specified as part of the test parameters). The test producer takes a clip from each of the songs, and saves the clip to a new folder of test clips. Before saving the clip back, it can optionally apply some distortion to the clip. These distortions, and their development, are discussed further in the evaluation (section \ref{evaluation:matching_with_distortion}).

The test producer finally passes a list of the test clips it has created to the next step, the tester, which passes each clip to the algorithm's matching function in the same way as was described for the prototype.

One important change made to the tester was to add the ability to automatically classify matches as correct or incorrect, by tagging each clip coming out of the test producer with its corresponding correct answer. Test results could then be output directly by the tester as a proportion of correct matches.


\subsection{Final refinements}

The shared code detailed in the section above was sufficient to verify that the Shazam algorithm was working well and could match the tested undistorted clips with 100\% accuracy.

A few further improvements were made during my implementation of the Philips algorithm. The primary improvement was to allow the matching process to use test sets -- that is, instead of the test producer choosing $n$ random songs each time, random subsets of the library are defined in advance as test sets, and all songs in a test set are used to produce test clips.

I wrote a new script, which produced multiple subsets of the library to draw tests from. It did this by copying songs to new test set folders, and only needed to be run once for each test set to be created. 

The test producer was modified to produce test clips from all songs in a given test set folder. I also added the functionality to run batches of tests from multiple test sets. This testing functionality is further discussed in the evaluation (section \ref{evaluation:matching_with_distortion}). 


\section{Implementation of the Shazam algorithm}
\label{section:shazam}

This section is a summary of my work in implementing the Shazam algorithm, which was proposed by Avery Wang in 2003 \cite{Wang03}. Section \ref{section:algoselection} contains more discussion on the rationale used to select the algorithms.

The original paper describes the algorithm as `capable of quickly identifying a short segment of music captured through a cellphone microphone in the presence of foreground voices and other dominant noise.' %TODO quote needed here?

I will describe development over the next four subsections in terms of the four main components of the algorithm. These correspond to the components discussed in section \ref{section:systemdesign}, namely: 

\begin{itemize}
  \item A \textbf{fingerprinting function} to calculate reproducible hash tokens from audio
  \item A \textbf{database} to store fingerprints for known songs
  \item A \textbf{registrar} to add fingerprints to the database
  \item A \textbf{matcher} to match unlabelled audio to labelled fingerprints in the database
\end{itemize}

Finally, one further subsection will describe the optimisations I applied to my implementation in order to improve performance.

\subsection{Fingerprinting function}

In order to identify signals in a robust manner, features of the audio which are relatively invariant in the face of noise and distortions must be used as a basis for fingerprinting.

\subsubsection{Constellation maps}

The fingerprinting function of the Shazam algorithm uses spectrogram peaks as its chosen features. A time-frequency point is chosen as a feature if it is a local energy maxima -- that is, it has more energy than any of its close neighbours.

In this way, a complex spectrogram can be reduced to a sparse set of time-frequency coordinates, which are relatively robust to signal degradations. The paper terms this list of coordinates a `constellation map', since it can resemble a star field. I will use this metaphor of stars and constellations throughout the rest of this section.

The pattern of dots in the constellation map for two matching pieces of audio should be the same: if you slide copies of each map over each other, there should be some time offset where all of the points overlap. In practice, not all of the points will match because distortions may have introduced or deleted points.

So far, the fingerprinting algorithm consists of computing a spectrogram for the input audio, finding peaks in it, and then storing these peaks to be matched against later. Matching becomes a process of trying to find the patch of `stars' in the database of constellation maps which most closely matches the small patch of stars calculated for the input clip. This process is discussed more in section \ref{shazam:matcher} but involves, for each peak in the input audio's constellation map, finding all peaks at the same frequency in the database. I will refer to these possible matches as `candidate matches'.

\subsubsection{Combinatorial hashing}

Unfortunately, finding matches in this way can be slow due to the low entropy of the constellation points. Using 1024 frequency bins results in a maximum of 10 bits of entropy available for locating candidate matches in the database. 

In order to increase this entropy and thus decrease the false match rate for candidate matches, Wang uses a combinatorial hashing technique. Each peak is paired with a number of other points in a target zone. The frequencies of both peaks and the time offset between them is packed into a 32-bit unsigned integer, which is returned as a hash value. Each hash value is also associated with the time offset from the start of the audio signal, although this does not make up part of the hash itself.

By again using 1024 possible frequency values, and another 10 bits for the time offset between the two peaks, each hash contains 30 bits of information as opposed to the previous 10. These 20 additional bits give each hash around a million times more specificity, and a similar search speedup. However, there is also a combinatorial explosion of the number of hashes. In order to limit this, a maximum fan-out factor $F$ is defined, and only the first $F$ peaks in the target zone are paired to form hashes. For $F=10$, as is used in my implementation, the end result is a speedup factor on the order of $10^5$ over the na{\"i}ve strategy described above.

\subsubsection{Implementation}

Computing the spectrogram of the audio was achieved using the \lstinline{spectrogram} function in the MATLAB Signal Processing Toolbox.

The second phase, finding the local maxima, is not well described in the original paper. As a prototype, my original working version of the algorithm used an open source peak finding function from the MATLAB Central website \cite{PeakFinder}. After confirming that this worked well, I decided to re-write the functionality so that the algorithm implementation was entirely my own code. After some research on methods for finding local maxima in 2D signals, informed by and based on the structure of the open source function I had already found, I settled on a three pass method. 

The first pass is a simple 3 by 3 median filter, which reduces high frequency noise. This pass also ensures that peaks with very small amplitudes, which are much more sensitive to distortion, are likely to be eliminated.

The second pass is a Gaussian filter which smooths the spectrogram, removing high frequency components of the time-frequency signal. Again, this removes some high frequency noise and also ensures that the peaks are reasonably spaced; peaks which are too close together are merged. The size of this Gaussian filter determines the feature size of the peaks detected. 

The final step of peak finding is to compare each point with its neighbours. If it is the largest of all its 8 neighbours, it is classified as a peak.

As described above, the peaks in the constellation map are then paired, and each pair is packed into a 32-bit integer hash. The list of hashes is then returned, along with their associated time offsets.


\subsection{Database}
\label{shazam:db}

As I discussed in section \ref{section:dbaccess}, the fingerprint databases use SQLite3. The database structure is straightforward, with a \lstinline{songs} table to store song metadata and a \lstinline{hashes} table to store fingerprint data.

The \lstinline{songs} table has two fields, \lstinline{song_id} and \lstinline{song_name}, and acts as a list of all songs in the database, along with an identifier for each one. In a real world implementation this table would almost certainly contain further metadata for each song, such as the artist, record label and album name. This extra information is not relevant to the algorithm itself and would have taken extra time to collect, so I omitted it.

The \lstinline{hashes} table has three fields: \lstinline{song_id}, \lstinline{hash}, and \lstinline{time}. The \lstinline{hash} field in each row stores frequency and time difference information for one pair of peaks, encoded as a 32-bit unsigned integer. The \lstinline{song_id} and \lstinline{time} fields store the identifier of the song from which the peaks were obtained, and the time offset (in samples) from the beginning of the song.


\subsection{Registrar}
\label{shazam:registrar}

The registrar is responsible for registering a new song in the database. It consists of a single function, which receives an audio filename and a handle to the database. 

It reads in the audio from the file, and resamples it to an 8kHz monophonic signal as a preprocessing step. It then calls the fingerprinter to obtain a fingerprint of the signal, and inserts the fingerprint hashes into the database, along with the song's name. Note that, for simplicity, the song's name is taken to be its filename, but in a real implementation metadata for the song including the name and artist would be used.


\subsection{Matcher}
\label{shazam:matcher}

The matching process is the final component of the system. It receives unlabelled audio and finds the closest matching song in the database. The implementation is described below in terms of the four main steps needed to match a clip of unlabelled audio.

\subsubsection{Fingerprinting of unlabelled audio}

The first step is to obtain the audio signal and calculate its fingerprint. In the case of my tests, the unlabelled audio signal is presented as a small audio file, but in a real system it might come directly from a microphone. The fingerprint is calculated by means of a call to the fingerprinting module.

\subsubsection{Finding candidate songs}

The next step is to find candidate songs in the database which might match the unlabelled audio. To do this, the matcher simply takes all of the hashes -- time-frequency peak pairs -- in the unlabelled audio's fingerprint, and searches for identical hashes in the database. This search is made efficient by means of an index on the \lstinline{hash} field of the database.

Each of these matches is binned according to the \lstinline{song_id}. This results in a list of candidate songs, each with a corresponding list of matching hashes. Each matching hash is associated with two time offsets: one  from the start of the song in the database, and one from the start of the unlabelled audio.

\subsubsection{Scoring candidate songs}

Once a list of candidate songs has been found, a distance score is needed for each one in order to determine which one is the closest match.

The list of hash matches corresponding to a candidate song can be represented using a scatterplot, with time in the candidate song along one axis and time in the unlabelled audio along the other. If two files match, there should be a sequence of features in one file that also occur in the same relative time sequence (minus some offset) in the other file. That is, one would expect to see a diagonal line in the scatterplot of matching hashes.

A robust regression technique could be used to detect this line, but that would be computationally expensive. To constrain the problem, the algorithm assumes that the gradient of the line is 1. This means the ability to detect clips played back faster or slower than the original song is lost, but in practice this is a rare distortion for most use cases.

Using this assumption, the corresponding times of matching features between matching signals have the relationship

\begin{align*}
  t_f' = t_f + k 
\end{align*}

where $t_f$ is the time of a feature $f$ in the unlabelled audio clip, and $t_f'$ is the time of the same feature in the candidate song. $k$ is the time offset between the sequences (the start time of the clip in the original song).

For each $(t_f',t_f)$ pair in the scatterplot, $\delta t_f$ is defined as

\begin{align*}
  \delta t_f = t_f' - t_f
\end{align*}

A histogram of these $\delta t_f$ values is plotted. If a diagonal line exists in the scatterplot, a sharp peak will be produced in the histogram at the associated offset (where $\delta t_f = k$).

The score assigned to each candidate song is simply the height of the highest peak in this histogram.


\subsubsection{Results}

The final task is to sort the candidate songs in descending order of score and return the top match. 

My implementation simply returns the candidate with the highest score. A more sophisticated implementation might attempt to assess the probability of a false match given the score of the top candidate, and return no match if the significance of the score is too low. However, this would require statistical analysis of the song library in question to determine false positive rates for different scores.

For the purposes of this project I decided that adding this feature for both implementations would require too much time, and would not contribute enough to the evaluation of the algorithms.


\subsection{Optimisations}
\label{section:shazamoptimisations}

At this point I had completed a working implementation of the Shazam algorithm. In preliminary tests with undistorted clips, it achieved a 100\% recognition rate, as would be hoped.

However, the run times of both processes -- registration and matching -- were much slower than I had hoped. Registering songs took around 70 seconds per song on average, and matching a 5 second clip took 30-60 seconds, depending on the number of candidates found in the database, which is quite variable.

Although speed of matching is not a primary consideration in evaluation, it is still relevant. Slow run times might also have prevented me from running as many tests as I wanted to. 

I knew that the algorithm had the potential to run very quickly, given that Wang reports search times of 5-500ms on a database of 20,000 tracks, implemented on a PC in 2003 \cite{Wang03}. These times are not directly comparable to mine, given the differences in implementation. My implementation runs using MATLAB and SQLite on a laptop, whereas scalability was one of Wang's main considerations so I would expect him to have used a more performance-oriented technology stack. However, five orders of magnitude still seemed like a large difference, particularly given the performance advances of commodity hardware over the last 12 years.

I decided to investigate whether any optimisations could help the performance. The three which I implemented are outlined below. All optimisations were benchmarked before and after by matching 20 random undistorted 5 second test clips, to quantify the increase in performance.

\subsubsection{Memory allocation}

During the fingerprinting process, a list of time-frequency peak pairs is created, which are then converted into hashes. These are stored in a two dimensional array, with each row of the array representing a peak pair. Each time a new peak pair is calculated, it is appended to this array.

Unfortunately, unlike languages such as Java, MATLAB does not have a native growable list type, so this append action copies the entire array to a new array with one more element each time. However, it is not possible to directly pre-allocate the array either, since it is impossible to know in advance how many peak pairs there will be.

Instead, I allocate an array with a certain starting size. This starting size is set to $n_s/200$ in my implementation, where $n_s$ is the number of samples in the song to be fingerprinted. The starting size attempts to guess how large the array will need to be, but need not be accurate. Each time the array becomes full, the size of the array is doubled. At the end of the append phase, the empty cells are removed from the end of the array.

This approach emulates the basic behaviour of Java's \lstinline{ArrayList} type. This simple change resulted in the fingerprinting process taking around half the time.

\subsubsection{Database queries}

Another area I decided to investigate was my database access, since it appeared to be a bottleneck, particularly in the case of inserting hashes when registering a song, and retrieving hashes during matching.

In the case of inserting hashes, many \lstinline{INSERT} queries are executed in the body of a loop: one per hash. Since there may be many thousands of hashes per song, this results in a large number of queries.

If no transaction is specified, an implicit transaction is wrapped around each query by the SQLite database. An overhead is associated with each transaction. In an attempt to reduce this overhead, I decided to try wrapping the loop (and thus, all of the insertion queries) in a single database transaction, replacing many thousands of transactions with just one. This sped up hash insertion by a factor of 15.

It was less clear how to proceed in increasing the performance of the database search. I had already created an index on the hash field being searched, which is the conventional wisdom on improving search query performance. I was unable to find any other solutions to speed up this process. My intuition is that this operation is near the limit of SQLite's performance, and that to achieve better speeds I would need to switch to a different database implementation.

\subsubsection{Database index}

Despite the fact that my optimisations used transactions to improve performance, I still felt that hash insertion was slower than I would expect. I also noticed that insertion became much slower as the database increased in size. I suspected that this was due to the index on the hash field, which is used to speed up search during the matching process.

I experimented with the index settings and verified that, without the index, insertion was approximately 20 times faster.

Removing the index altogether was not an option given its importance in the speed of search queries. However, I found that by deleting the index before doing a batch insertion of hashes, and rebuilding the index at the end of the process, significant time savings could be made. Rebuilding the index takes approximately 90 seconds, but this becomes a worthwhile trade if more than around 15 songs are being registered (assuming the songs are about 4 minutes in length).

One possible problem with this approach is that if the registration process is interrupted, the database could be left in a bad state, without the index built. To mitigate this situation, I added a check at the start of the matching process to ensure that the index exists, and rebuild it if necessary.

\subsubsection{Conclusion}

With this combination of optimisations, I increased performance significantly, to the point where I was happy to move on to the second algorithm. The final implementation takes, on average, 6.3 seconds to register a song and 4.4 seconds to match a 5 second clip.

Although further improvements could probably be made, I decided that this was a good middle ground between having an implementation that was fast enough that it could be tested quickly, and not spending all of my project time chasing small performance improvements.


\section{Implementation of the Philips algorithm}
\label{section:philips}

Once implementation of the Shazam algorithm was complete, I moved my focus to the Philips algorithm, proposed in 2001 by Haitsma et al. \cite{Haitsma02}. 

This section is a summary of the work completed during its implementation. It will follow a similar structure to my description of the first algorithm's implementation, with one subsection for each module implemented, and a final one describing performance optimisations.


\subsection{Fingerprinting function}
\label{philips:fingerprinter}

\subsubsection{Framing}

The Philips algorithm works using a \textit{fingerprint stream} approach, in which small sections of audio, referred to as \textit{frames}, are each assigned a 32-bit \textit{sub-fingerprint}. This stream of sub-fingerprints comprises the fingerprint. Sub-fingerprints are too small to uniquely identify a frame, but a longer interval containing sufficiently many frames can be used to identify audio robustly.

The frames of audio are overlapping, and are weighted using a Hann window, with overlap factor 31/32. This large overlap factor ensures that even if a comparison clip's frames are misaligned by 50\% of the frame size, the frames will be similar enough to enable identification. Due to the large overlap, subsequent sub-fingerprints are very similar and vary fairly slowly with time. 

Note that although the Shazam algorithm appears to take a very different approach to this algorithm, the framing process is in fact very similar. The Shazam algorithm's framing process is implicit within its computation of a spectrogram as the first step. A Hann window with a large overlap factor is also used in the Shazam algorithm, for the same reasons.

\subsubsection{Sub-fingerprints}

The next step of the Philips algorithm is to perform a Fourier transform on each frame, to transfer it into the frequency domain where the important perceptual features can be found.

In order to compute the 32-bit sub-fingerprint for each frame, 33 non-overlapping frequency bands are selected. These lie in the 300Hz to 2000Hz range, which is the most relevant range for the human auditory system. The human auditory system is based approximately on logarithmically spaced bands -- that is, a sound one octave higher in pitch has double the frequency. As such, the bands in the algorithm are also chosen to be logarithmically spaced.

The energy in each frequency band is summed up over a frame to give 33 energy values. The energy of each band is compared with the energy of the band below, and the same two bands in the subsequent frame, to compute a single bit:

\begin{align*}
  F(n,m)=
  \begin{cases}
      1 & \text{if } E(n,m)-E(n,m+1) > E(n-1,m)-E(n-1,m+1)\\
      0 & \text{otherwise}
  \end{cases}
\end{align*}

Here $F(n,m)$ is the $m$th bit of the $n$th sub-fingerprint, and $E(n,m)$ is the energy of the $m$th frequency band of the $n$th frame. Note that although there are 33 frequency bands, only 32 bits can be calculated because each bit depends on two bands.


\subsection{Database}

The database for this algorithm is very similar to the one described in section \ref{shazam:db} for the Shazam algorithm. There are again two tables, a \lstinline{songs} table to store song metadata and a \lstinline{hashes} table to store fingerprint data.

The \lstinline{songs} table has two fields, \lstinline{song_id} and \lstinline{song_name}, and acts as a list of all songs in the database, along with an identifier for each one.

The \lstinline{hashes} table has three fields: \lstinline{song_id}, \lstinline{hash}, and \lstinline{hash_number}. The \lstinline{hash} field stores sub-fingerprint values, packed as 32-bit unsigned integers. The \lstinline{song_id} and \lstinline{hash_number} fields store the identifier of the song, and number of the sub-fingerprint from the start of the song (i.e. the $n$ used in $F(n,m)$ above).

There is an index on the \lstinline{hash} field for fast searching, as in the Shazam database. However, there is also a second index on the \lstinline{hash_number} field, because the search process for this algorithm also requires the ability to quickly recall sequences of hash values based on their hash numbers. The matching process is discussed in detail in section \ref{philips:matcher}.


\subsection{Registrar}

The Philips algorithm's registration process works identically to the Shazam algorithm's counterpart, which is described in section \ref{shazam:registrar}.


\subsection{Matcher}
\label{philips:matcher}

As in the Shazam algorithm, the matcher receives unlabelled audio, and attempts to find a close match in the database. The first step is to calculate the fingerprint of the unlabelled audio, which is achieved through a call to the fingerprinting module described in section \ref{philips:fingerprinter}.

\subsubsection{Fingerprint similarity}

In the Philips algorithm, fingerprints are represented by a stream of 32 bit sub-fingerprints, each of which is based on a few milliseconds of the song. On its own, a single sub-fingerprint has insufficient discriminating power to uniquely identify that section of audio, so robust identification must be based on a sequence of sub-fingerprints.

Two sequences of sub-fingerprints are said to be similar if corresponding bits in the sequences are similar. More formally, the distance between the two sequences is defined to be the Hamming distance between their respective bit sequences. A smaller distance means a higher similarity between the sequences, which in turn implies a higher similarity between the corresponding audio clips.


\subsubsection{Searching for a match}

When an unlabelled clip is entered into the system, and its fingerprint calculated, the next step is to locate the closest match for the given fingerprint in the database. The input clip will be a subset of the song, usually just a few seconds.

The database contains a stream of sub-fingerprints for each song in the library. To find a match, the algorithm must find a subset of one of these streams such that the distance between it and the unlabelled clip's fingerprint is minimised. This can be visualised as sliding the unlabelled clip's fingerprint over the database, finding the location where it aligns most closely.

The na\"ive implementation would be a linear search, and would be very slow for large databases. To speed up the search, an assumption is made about the characteristics of the sub-fingerprints under distortion. 

The assumption made is that at least one of the sub-fingerprints from the unlabelled audio will exactly match its counterpart in the database. In other words, at least one of the sub-fingerprints will be unchanged under whatever distortions are applied.

This assumption significantly increases the speed of search, since only locations with a perfectly matching sub-fingerprint must be considered. This comes at the risk of falsely rejecting positions in the database which might have been the closest match, which will reduce the accuracy of the system under distortion.

The full matching process proceeds as follows:

\begin{enumerate}

  \item \textbf{Calculation of the unlabelled audio's fingerprint.} This is done by the fingerprinting module.

  \item \textbf{Finding candidate starting positions.} Each sub-fingerprint from the unlabelled audio's fingerprint is searched for in the database. Anywhere there is a matching sub-fingerprint, the song identifier and hash number are collected. The sub-fingerprint's position in the unlabelled audio is subtracted from the hash number to find a possible starting point for the unlabelled clip in the database. Note that duplicates must be removed from this list in case two or more sub-fingerprints were perfectly preserved.

  \item \textbf{Retrieving candidate sequences.} Once candidate starting positions have been found, a subset of the fingerprint stream, starting from each of those positions, is retrieved. The length of this subset is the same as the length of the unlabelled audio's fingerprint stream.

  \item \textbf{Scoring candidates.} Each candidate sequence is compared to the unlabelled audio's fingerprint, using the Hamming distance measure described above.

  \item \textbf{Returning the best candidate.} Finally, the position of the candidate sequence with the smallest distance is returned as the best match.

\end{enumerate}

\subsection{Optimisations}

Just as with my implementation of the Shazam algorithm, my first working implementation was slower than I would have liked. Matching a 5 second clip took 30-60 seconds, and registration took around 10 seconds. 

All of the same optimisations identified for the Shazam algorithm could be applied again. I will briefly summarise the optimisations as applied to this algorithm, but I will not repeat the discussion in section \ref{section:shazamoptimisations}, which describes their specifics in more detail.


\subsubsection{Memory allocation}

During the matching process, a list of candidate positions is created. However, there is no way of knowing how many candidates there will be in advance, so the list cannot be pre-allocated, resulting in the list being copied unnecessarily many times. The same technique used in the Shazam algorithm of doubling the list's size when it becomes full is applied to improve the performance of append operations.

This change improved fingerprinting performance by a factor of approximately 1.5.

\subsubsection{Database queries}

As in the Shazam algorithm, database access is the biggest bottleneck, particularly during matching. When inserting a song's fingerprint, all of the insertion queries for each sub-fingerprint are batched into a single transaction to decrease the transaction overheads. 

This sped up hash insertion by a factor of approximately 20.

To speed up database search, an index is created on the \lstinline{hash} field, as with the Shazam algorithm. This speeds up finding candidate positions, but not the retrieval of fingerprint subsequences for scoring of candidate positions. In order to improve performance of the latter, I also added an index on the \lstinline{hash_number} field.

This sped up finding candidate positions and retrieval of fingerprint subsequences by a factor of 15-20.


\subsubsection{Database index}

As with the Shazam algorithm, the indexes added for query optimisation degraded the performance of insertion operations. However, since registration of songs (which is the only process which inserts into the database) happens in batches, the indexes can be dropped before a batch insertion and rebuilt afterwards.

This speeds up the database insertion by a factor of almost 20, although rebuilding the index afterwards adds around 100 seconds.

As before, checks were added to make sure the indexes are always rebuilt before they are required, even if the registration process is stopped in the middle of a batch.


\subsubsection{Conclusion}

A significant performance gain was again achieved for the second algorithm using these optimisations. The final implementation takes, on average, 12.9 seconds to register a song and 2.5 seconds to match a 5 second clip.

I decided, again, that the performance was both sufficient and as good as could reasonably be expected given the time constraints and technology choices. I decided at this point to proceed to the evaluation phase.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Evaluation}
\label{evaluation}

With both implementations complete, I moved on to the evaluation phase. This section first discusses evaluation of both algorithms with respect to their matching accuracy in the presence of several types of distortions. I will then move on to talk briefly about the performance of the two implementations, and conclude with my own comments on the strengths and weaknesses of each algorithm.


\section{Matching in the presence of distortions}
\label{evaluation:matching_with_distortion}

The process for testing robustness of matching is discussed in my implementation of the shared matching code in section \ref{section:sharedimpl}. It consists of inputting clips of songs from the library, and checking whether each clip is matched to the correct song. These clips may have different distortions applied to them to test robustness.

The test clips are drawn from three test sets, each of which is a random subset of the song library. All tests are run once for each test set, and the three results are averaged. There are 72 songs in each test set, with each one representing 10\% of the full library of 720.

Three main categories of test clips were chosen: plain clips, clips with added noise, and re-recorded clips. They are explained in the next three sections, accompanied by the relevant results for each algorithm.


\subsection{Plain clips}

Plain clips are simply generated by cropping original song audio files to the required length, and were used as a first check during development to make sure the algorithm is functioning at a basic level. The match rate for these should be very close to 100\%.

All of my plain test clips are 5 seconds in length. As would be expected, both the Shazam and Philips algorithms score 100\% on this test.


\subsection{Clips with added noise}

Clips with added noise are generated by taking plain clips and overlaying noise at different signal to noise ratios (SNR's). The noise added might be generated, such as Gaussian noise, or taken from a pre-recorded noise file. An example of pre-recorded noise might be a clip of ambient noise in a busy restaurant.

As well as the length of the test clip, these tests also allow me to test at different SNR values. I decided on clip lengths of 5, 10 and 15 seconds, and SNR values ranging from -9dB to +9dB, at intervals of 3dB. All tests were run for all possible pairs of these parameters, giving 21 different test setups in total for each noise type. 

In order to achieve the correct SNR, the noise is first generated at an arbitrary level. The total power is computed for both the noise and the original clip, and the amplitude of the noise clip is multiplied so as to give the correct ratio between the two signal powers. The two signals are then added, and normalised to lie between -1 and 1 to avoid clipping artefacts where samples with a magnitude higher than 1 are clamped to a magnitude of 1.

\begin{figure}[p]
    \centering

    \begin{subfigure}{\textwidth}
      \includegraphics[width=\textwidth]{./figs/gaussian_philips_results.eps}
    \end{subfigure}

    \vspace{10mm}

    \begin{subfigure}{\textwidth}
      \includegraphics[width=\textwidth]{./figs/gaussian_shazam_results.eps} 
    \end{subfigure}

    \vspace{15mm}

    \caption{Results for additive noise tests on both algorithms, using Gaussian white noise. Each line represents a different clip length, and each point is the averaged result over three tests.}
    \label{fig:gaussian_results}
\end{figure}


\subsubsection{Gaussian white noise}

One simplistic form of noise is Gaussian white noise. This is created by sampling noise from a Gaussian distribution, provided by MATLAB's \lstinline{randn} function. Each sample of noise is independent of all the other samples, which is the requirement for white noise.

This type of noise is not very realistic for the use case of using a mobile phone to record a song clip, but white noise is a component of the noise often introduced in radio broadcasts. A user wishing to monitor songs on the radio might want to use an algorithm robust to white noise.

Results for the Gaussian white noise test can be seen in the two graphs in Figure \ref{fig:gaussian_results}. 

The Shazam algorithm clearly outperforms the Philips algorithm in this test -- the average recognition rate over the three test sets is higher in every case than for the Philips algorithm, although it should be noted that the difference between the two algorithms is within one standard deviation for the SNRs over 6db.

For the Shazam algorithm, longer clip lengths reliably translate to higher recognition rates, although the effect diminishes for higher SNR values. Interestingly, this effect is much less pronounced in the results for the Philips algorithm. There is a small correlation between clip length and recognition rate for SNR values of 0db and below, but the effect appears to actually be reversed for higher SNR values: longer clips result in a slightly lower recognition rate. This is possibly a result of the Hamming distance metric used: whilst the Shazam algorithm simply rewards similarities in the fingerprints, the Philips algorithm penalises differences in the fingerprints. A longer clip may allow for a higher chance of encountering a patch of noise which significantly penalises an otherwise closely matching clip.

The differences for different clip lengths are all within 2 standard deviations for the Philips algorithm, so further testing would probably be needed to confirm the trend. However, the clear correlation seen in the case of the Shazam algorithm is not present.

\begin{figure}[p]
    \centering

    \begin{subfigure}{\textwidth}
      \includegraphics[width=\textwidth]{./figs/natural_philips_results.eps}
    \end{subfigure}

    \vspace{10mm}

    \begin{subfigure}{\textwidth}
      \includegraphics[width=\textwidth]{./figs/natural_shazam_results.eps} 
    \end{subfigure}

    \vspace{15mm}

    \caption{Results for additive noise tests on both algorithms, using real world noise. Each line represents a different clip length, and each point is the averaged result over three tests.}
    \label{fig:natural_results}
\end{figure}

\subsubsection{Real world noise}

To address the use case of a user identifying music using their mobile phone, I also ran tests using real world noise. I obtained a public domain clip of some ambient noise recorded inside a shopping centre from the website Sound Bible \cite{ShoppingAmbience}.

This noise was added to plain clips at different signal to noise ratios to simulate the effect of trying to identify a song over the noise of a busy environment. Results for the tests can be seen in the two graphs in Figure \ref{fig:natural_results}.

Again, the Shazam algorithm outperforms the Philips algorithm. Recognition rates are lower, in both cases, than for the additive Gaussian noise test, indicating that both algorithms deal with white noise more easily than the shopping centre noise. This is probably a result of the fact that white noise has a roughly flat frequency spectrum, and so is likely to have less effect on the features that both algorithms try to detect, which live in the frequency domain. Conversely, the ambient noise has many peaks and troughs in its spectrogram, which could be confused for features of the music clips.

Once again, the Shazam algorithm performs consistently better when longer clip lengths are used, whilst the Philips algorithm only shows this effect for lower SNR values.

In conclusion, these results are consistent with the previous results, although recognition rates are lower across the board when real world noise is added instead of Gaussian noise.


\subsection{Re-recorded clips}

Re-recorded clips were generated by playing a plain clip out of some speakers and re-recording the audio through a microphone. This captures more naturalistic distortions such as echo and reverb, as well as any inaccuracies, frequency biases or clipping introduced by the microphone or speakers. This is the most naturalistic test that I performed, and is very close in its characteristics to the mobile phone use case.

I used a pair of reasonably high quality hi-fi speakers, and the microphone built into my laptop (a 2014 Macbook Pro). The microphone is fairly low quality and is quite similar to a standard smartphone microphone. Tests were carried out in my room, during the Easter vacation to minimise noise from neighbours. Minimal background noise was present in the final test clips -- the majority of signal degradation was due to the acoustic characteristics of the speakers, microphone and room. The results can be seen in Figure \ref{fig:re-recorded_results}. 

Once again, the Shazam algorithm exhibits a strong correlation between longer clip lengths and higher accuracy, whilst the Philips algorithm does not. The Philips algorithm's accuracy increases in variability as the clip length increases. This supports my hypothesis that longer clips increase the chance of a patch of high distortion which is over-penalised by the Philips algorithm.

The Shazam algorithm does not outperform the Philips algorithm by as much of a margin here as in previous tests, and actually performs slightly worse for 5 second clips. 

\begin{figure}[htbp!]
    \centering

    \begin{subfigure}{0.5\textwidth}
    	\centering
     	\includegraphics[width=0.95\linewidth]{./figs/barcoder_passthrough.eps}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.5\textwidth}
    	\centering
     	\includegraphics[width=0.95\linewidth]{./figs/constellation_passthrough.eps} 
    \end{subfigure}

    \caption{Results for re-recorded tests on both algorithms. Each point is the averaged result over three tests.}
    \label{fig:re-recorded_results}
\end{figure}

\section{Performance}

There are two aspects to the performance of the algorithms: matching, and registration. Of the two, matching is more important since registration is a one-time cost for each song in the database, whilst matching occurs in every application of the algorithm. 

For registration, fingerprinting is the longest process for both algorithms, since the whole song must be analysed. Other time consuming processes include reading in the song audio from disc and resampling it to a standard sample rate monophonic signal, and inserting the fingerprints into the database.

The audio processing is approximately the same for each algorithm, and so just adds an extra constant cost to each song processed. Database insertion is roughly proportional to the size of the fingerprints produced, since both algorithms use the same database architecture and have similar schemas. As such, the timing differences for registration are mostly a result of differences in fingerprinting speed and size.

To measure the performance of the registration processes, I added the full library of songs to an empty database, and timed the process. As can be seen in Table \ref{table:registration_speed}, registration takes around double the time per song for the Philips algorithm. There is no significant difference in the time taken to rebuild the indexes after a batch insertion.

\newcolumntype{L}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\begin{table}[t]
	\begin{tabularx}{\textwidth}{L{2.7cm}L{4cm}L{4cm}L{4cm}}
		\textbf{Algorithm} & \textbf{Registration time per song (s)} & \textbf{Time to re-build indexes (s)} & \textbf{Registration time for 720 songs (s)}\\ 
		\hline 
		Shazam & 6.3 & 97 & 4634\\ 
		Philips & 12.9 & 99 & 9399\\ 
	\end{tabularx}
	\caption{Registration speeds for both algorithms. Time per song does not include time for re-building indexes.}
	\label{table:registration_speed}
\end{table}


The performance of the matching process is dominated by the database search, since it involves many distance calculations between fingerprints, and a lot of database access. Fingerprinting speed is a less important factor, since input clips are much shorter than full songs.

To measure the speed of matching, I ran the matching test for 5 second clean clips, and timed the process. This test includes 216 clips in total. The mean time taken per song can be seen in Table \ref{table:registration_speed}.

Here the Philips algorithm outperforms the Shazam algorithm by a factor of 1.8, taking less than 2.5 seconds to match each clip on average.

\newcolumntype{L}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\begin{table}[h]
	\centering
	\begin{tabularx}{0.75\textwidth}{L{4cm}L{5.5cm}}
		\textbf{Algorithm} & \textbf{Mean matching time for 5 second clean clip (s)} \\ 
		\hline 
		Shazam & 4.45\\ 
		Philips & 2.47\\ 
	\end{tabularx}
	\caption{Matching speeds for both algorithms.}
	\label{table:registration_speed}
\end{table}


\section{Database size}

The sizes of both fingerprint databases are shown in Table \ref{table:db_size}.

Although the two algorithms have very different fingerprinting techniques, they have the same schema for their fingerprint tables, since the hash values are packed into 32-bit integers in both cases. The Shazam algorithm has around 50\% more entries in its databases, indicating that the fingerprints it produces are significantly larger. 

However, observation of the actual size of the database on disc reveals that the Philips database is around 14\% larger. This can be accounted for by the fact that it has an extra index on the fingerprints table to improve its database search speed.

\newcolumntype{L}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\begin{table}[ht]
	\begin{tabularx}{\textwidth}{L{3cm}L{4cm}L{4cm}L{4cm}}
		\textbf{Algorithm} & \textbf{Total database size (MB)} & \textbf{Database size per song (MB)} & \textbf{Entries in fingerprint table}\\ 
		\hline
		Shazam & 865.2 & 1.20 & 26,078,196\\ 
		Philips & 989.2 & 1.37 & 16,975,422\\ 
	\end{tabularx}
	\caption{Database sizes for both algorithms}
	\label{table:db_size}
\end{table}


\section{Overall comparison}

I will conclude the evaluation section with a brief summary of the two algorithms' comparative strengths and weaknesses.

The Shazam algorithm outperforms the Philips algorithm on pure accuracy fairly consistently, with the exception of the shorter clips on the re-recorded clips test. It also has a slightly smaller fingerprint database. In applications where distortions are likely and accuracy is important, or storage space is very limited, the Shazam algorithm is probably a better choice.

However, the Philips algorithm wins on matching speed by a significant margin, and still has comparable accuracy on shorter clips in some tests. In applications where matching speed is important, clips are limited to 5 seconds or less, or distortions are unlikely, the Philips algorithm may be superior. 

The Shazam algorithm is, in my opinion, a better general purpose recognition algorithm for most use cases. However, the Philips algorithm may be better in some less common cases such as content watermarking, due to its speed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}
\label{conclusion}

In this project I successfully implemented two algorithms for audio fingerprinting and demonstrated their use for song recognition. I gathered a library of songs to test against, and used it to evaluate and compare both algorithms against a variety of criteria. The implementations performed well given the constraints of the technologies used, and my evaluation successfully found strengths and weaknesses in both.

Had more time been available, I would have liked to have attempted implementation of a third algorithm which strayed more from the standard structure of an audio fingerprinting algorithm. The two algorithms examined had many similarities, mainly as a result of the similarity of their aims. For example, an algorithm that attempts to recognise covers of songs as well as exact recordings would be interesting to study.

If I were given the chance to repeat the project, I might have tried to devote a little more time to testing. It would have been nice to have time to include tests for different levels of MP3 compression, for example, or tests that combine multiple distortions. The latter of these would have been particularly interesting -- for instance, does applying noise followed by compression result in better accuracy than compression followed by noise?  

I might also have chosen a language other than MATLAB for my implementation. In hindsight, the convenience of inbuilt graph plotting and signal processing functionality may have been outweighed by my unfamiliarity with the language. Choosing another language would have also given me the opportunity to experiment with other database implementations to try to improve performance. However, I am broadly happy with the technology and implementation choices I made.

%todo more?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix



\chapter{Full matching test results}

In all tables, clip length is abbreviated as CL and standard deviation as SD.

% Centering for X columns
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\section{Basic clips}

For both the Shazam and Philips algorithms, plain clips of length 5 seconds were tested. For all 3 test sets, both algorithms matched all 72 clips correctly.

\section{Gaussian white noise}

\subsubsection{Philips}

\begin{tabularx}{\textwidth}{YYYYYYY}
\hline
&  & \multicolumn{5}{c}{Correctly matched (out of 72)} \\ 
\cline{3-7}
CL (s) & SNR (dB) & Set A & Set B & Set C & Mean & SD\\ 
\hline
\hline
5 & -9 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
5 & -6 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
5 & -3 & 6 & 10 & 8 & 8.0 & 2.0\\ 
\hline
5 & 0 & 45 & 44 & 43 & 44.0 & 1.0\\ 
\hline
5 & 3 & 70 & 67 & 69 & 68.7 & 1.5\\ 
\hline
5 & 6 & 70 & 71 & 72 & 71.0 & 1.0\\ 
\hline
5 & 9 & 72 & 71 & 72 & 71.7 & 0.6\\ 
\hline
\hline
10 & -9 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
10 & -6 & 0 & 1 & 1 & 0.7 & 0.6\\ 
\hline
10 & -3 & 10 & 14 & 9 & 11.0 & 2.6\\ 
\hline
10 & 0 & 52 & 44 & 48 & 48.0 & 4.0\\ 
\hline
10 & 3 & 67 & 66 & 65 & 66.0 & 1.0\\ 
\hline
10 & 6 & 69 & 69 & 71 & 69.7 & 1.2\\ 
\hline
10 & 9 & 69 & 71 & 72 & 70.7 & 1.5\\ 
\hline
\hline
15 & -9 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
15 & -6 & 1 & 1 & 1 & 1.0 & 0.0\\ 
\hline
15 & -3 & 9 & 8 & 8 & 8.3 & 0.6\\ 
\hline
15 & 0 & 47 & 48 & 45 & 46.7 & 1.5\\ 
\hline
15 & 3 & 67 & 58 & 67 & 64.0 & 5.2\\ 
\hline
15 & 6 & 71 & 69 & 68 & 69.3 & 1.5\\ 
\hline
15 & 9 & 70 & 68 & 72 & 70.0 & 2.0\\ 
\hline
\end{tabularx}

\subsubsection{Shazam}

\begin{tabularx}{\textwidth}{YYYYYYY}
\hline
&  & \multicolumn{5}{c}{Correctly matched (out of 72)} \\ 
\cline{3-7}
CL (s) & SNR (dB) & Set A & Set B & Set C & Mean & SD\\ 

\hline
\hline
5 & -9 & 5 & 6 & 7 & 6.0 & 1.0\\ 
\hline
5 & -6 & 24 & 25 & 29 & 26.0 & 2.6\\ 
\hline
5 & -3 & 60 & 54 & 60 & 58.0 & 3.5\\ 
\hline
5 & 0 & 70 & 68 & 70 & 69.3 & 1.2\\ 
\hline
5 & 3 & 70 & 71 & 71 & 70.6 & 0.6\\ 
\hline
5 & 6 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
5 & 9 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
\hline
10 & -9 & 13 & 7 & 17 & 12.3 & 5.0\\ 
\hline
10 & -6 & 49 & 45 & 47 & 47.0 & 2.0\\ 
\hline
10 & -3 & 70 & 69 & 71 & 70.0 & 1.0\\ 
\hline
10 & 0 & 72 & 72 & 71 & 71.7 & 0.6\\ 
\hline
10 & 3 & 71 & 72 & 71 & 71.3 & 0.6\\ 
\hline
10 & 6 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
10 & 9 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
\hline
15 & -9 & 20 & 12 & 18 & 16.7 & 4.2\\ 
\hline
15 & -6 & 55 & 59 & 58 & 57.3 & 2.1\\ 
\hline
15 & -3 & 70 & 71 & 71 & 70.7 & 0.6\\ 
\hline
15 & 0 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
15 & 3 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
15 & 6 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
15 & 9 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline

\end{tabularx}

\section{Real world noise}

\subsubsection{Philips}
\begin{tabularx}{\textwidth}{YYYYYYY}
\hline
&  & \multicolumn{5}{c}{Correctly matched (out of 72)} \\ 
\cline{3-7}
CL (s) & SNR (dB) & Set A & Set B & Set C & Mean & SD\\ 
\hline
\hline

5 & -9 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
5 & -6 & 0 & 1 & 0 & 0.3 & 0.6\\ 
\hline
5 & -3 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
5 & 0 & 3 & 1 & 1 & 1.7 & 1.2\\ 
\hline
5 & 3 & 23 & 22 & 23 & 22.7 & 0.6\\ 
\hline
5 & 6 & 60 & 59 & 59 & 59.3 & 0.6\\ 
\hline
5 & 9 & 71 & 70 & 70 & 70.3 & 0.6\\ 
\hline
\hline
10 & -9 & 1 & 0 & 0 & 0.3 & 0.6\\ 
\hline
10 & -6 & 1 & 0 & 1 & 0.7 & 0.6\\ 
\hline
10 & -3 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
10 & 0 & 7 & 1 & 4 & 4.0 & 3.0\\ 
\hline
10 & 3 & 32 & 26 & 25 & 27.7 & 3.8\\ 
\hline
10 & 6 & 57 & 58 & 58 & 57.7 & 0.6\\ 
\hline
10 & 9 & 63 & 64 & 67 & 64.7 & 2.1\\ 
\hline
15 & -9 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
15 & -6 & 0 & 1 & 0 & 0.3 & 0.6\\ 
\hline
15 & -3 & 0 & 1 & 0 & 0.3 & 0.6\\ 
\hline
15 & 0 & 8 & 1 & 5 & 4.7 & 3.5\\ 
\hline
15 & 3 & 33 & 29 & 28 & 30.0 & 2.6\\ 
\hline
15 & 6 & 47 & 52 & 53 & 50.7 & 3.2\\ 
\hline
15 & 9 & 61 & 60 & 66 & 62.3 & 3.2\\ 
\hline


\end{tabularx}

\subsubsection{Shazam}
\begin{tabularx}{\textwidth}{YYYYYYY}
\hline
&  & \multicolumn{5}{c}{Correctly matched (out of 72)} \\ 
\cline{3-7}
CL (s) & SNR (dB) & Set A & Set B & Set C & Mean & SD\\ 

\hline
\hline
5 & -9 & 1 & 0 & 0 & 0.3 & 0.6\\ 
\hline
5 & -6 & 2 & 2 & 0 & 1.3 & 1.2\\ 
\hline
5 & -3 & 13 & 14 & 13 & 13.3 & 0.6\\ 
\hline
5 & 0 & 48 & 52 & 53 & 51.0 & 2.6\\ 
\hline
5 & 3 & 71 & 68 & 68 & 69.0 & 1.7\\ 
\hline
5 & 6 & 71 & 72 & 72 & 71.7 & 0.6\\ 
\hline
5 & 9 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
\hline
10 & -9 & 0 & 0 & 0 & 0.0 & 0.0\\ 
\hline
10 & -6 & 5 & 4 & 0 & 3.0 & 2.6\\ 
\hline
10 & -3 & 23 & 34 & 25 & 27.3 & 5.9\\ 
\hline
10 & 0 & 67 & 66 & 63 & 65.3 & 2.1\\ 
\hline
10 & 3 & 72 & 72 & 71 & 71.7 & 0.6\\ 
\hline
10 & 6 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
10 & 9 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
\hline
15 & -9 & 1 & 1 & 0 & 0.7 & 0.6\\ 
\hline
15 & -6 & 7 & 10 & 2 & 6.3 & 4.0\\ 
\hline
15 & -3 & 42 & 41 & 40 & 41.0 & 1.0\\ 
\hline
15 & 0 & 70 & 69 & 69 & 69.3 & 0.6\\ 
\hline
15 & 3 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
15 & 6 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline
15 & 9 & 72 & 72 & 72 & 72.0 & 0.0\\ 
\hline

\end{tabularx}

\newpage
\section{Re-recorded clips}

\subsubsection{Philips}
\begin{tabularx}{\textwidth}{YYYYYYY}
\hline
&  \multicolumn{5}{c}{Correctly matched (out of 72)} \\ 
\cline{2-6}
CL (s) & Set A & Set B & Set C & Mean & SD\\ 

\hline
\hline
5 & 33 & 34 & 33 & 33.3 & 0.6\\ 
\hline
10 & 54 & 40 & 45 & 46.3 & 7.1\\ 
\hline
15 & 44 & 36 & 32 & 37.3 & 6.1\\ 
\hline

\end{tabularx}

\bigskip

\subsubsection{Shazam}
\begin{tabularx}{\textwidth}{YYYYYYY}
\hline
&  \multicolumn{5}{c}{Correctly matched (out of 72)} \\ 
\cline{2-6}
CL (s) & Set A & Set B & Set C & Mean & SD\\ 

\hline
\hline
5 & 34 & 31 & 33 & 32.6 & 1.5\\ 
\hline
10 & 54 & 52 & 58 & 54.7 & 3.1\\ 
\hline
15 & 60 & 60 & 60 & 60 & 0\\ 
\hline

\end{tabularx}



\chapter{Project Proposal}


\input{proposal}

\end{document}
